{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mykolesiko/eeg_investigation/blob/diplom/MADE_PSD_leak_beta1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cejRPk3ccbt1"
      },
      "source": [
        "Статья CONVOLUTIONAL NEURAL NETWORK APPROACH FOR EEG-BASED EMOTION\n",
        "RECOGNITION USING BRAIN CONNECTIVITY AND ITS SPATIAL INFORMATION\n",
        "https://drive.google.com/file/d/1KU_5ovXNHyrt15iZnm7tmZMoxxGltN_J/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zR9HY1qccEc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from  scipy import stats\n",
        "import scipy\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlNsBL96YiCD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/MADE/Project/deap\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEw0N3rmYkQ_"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  def __init__(self): #(data_path, epoch, batch_siz, image_size, learning_rate, weight_deca, learning_rate, learning_rate_gamma, weight_bce, load, output_dir)\n",
        "    self.data_path = \"/content/drive/MyDrive/MADE/semester2/CV/contest02/data/\"\n",
        "    self.epochs = 2\n",
        "    self.batch_size = 256\n",
        "    self.lr= 3e-4\n",
        "    self.weight_decay= 1e-6\n",
        "    self.learning_rate=None\n",
        "    self.learning_rate_gamma=None\n",
        "    self.weight_bce=1\n",
        "    self.load=None\n",
        "    self.output_dir=\"/content/drive/MyDrive/MADE/Project/RACNN_models/\"\n",
        "    self.data_dir =\"./data_preprocessed_python/\"# \"/content/drive/MyDrive/MADE/Project/train/physionet.org/\"\n",
        "args = Args()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uul6VvjtYoFb"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import scipy\n",
        "from  scipy import signal\n",
        "from scipy.fft import fft, fftfreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1MO2eTqY0fb"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "data_dir = './data_preprocessed_python'\n",
        "files = glob.glob(os.path.join(data_dir, \"*.dat\"))\n",
        "data_raw = []\n",
        "for file_data in files:\n",
        "    raw_data = pickle.load(open(file_data, 'rb'), encoding='latin1')\n",
        "    data.append(raw_data['data'])\n",
        "    labels.append(raw_data['labels'])\n",
        "    #break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmm_HCogZB-I"
      },
      "outputs": [],
      "source": [
        "labels_bin = []\n",
        "for sub in range(32):\n",
        "#for sub in range(1):\n",
        "  temp = labels[sub] >= 4.5\n",
        "  labels_bin.append(temp)\n",
        "  #print(sum(labels_bin[sub][:, type_emotion]), end=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-0rO9Vy0xqa",
        "outputId": "b17f0874-5d1c-469d-c983-4d138363c2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZhJBeZQZiI1"
      },
      "outputs": [],
      "source": [
        "LEN_RECORD_IN_SECONDS = 60\n",
        "NVIDEOS = 40\n",
        "HCANALS = 9\n",
        "WCANALS = 9\n",
        "NTIMES_IN_SAMPLE = 128\n",
        "NTIMES_IN_SEC = 128\n",
        "NCANALS = 32\n",
        "NFEATURES = 32\n",
        "NSUBJECTS = 32\n",
        "electrode_matrix = {}\n",
        "electrode_matrix['FP1'] = [0, 3]\n",
        "electrode_matrix['FP2'] = [0, 5]\n",
        "electrode_matrix['AF3'] = [1, 3]\n",
        "electrode_matrix['AF4'] = [1, 5]\n",
        "electrode_matrix['F7']  = [2, 0]\n",
        "electrode_matrix['F3']  = [2, 2]\n",
        "electrode_matrix['FZ']  = [2, 4]\n",
        "electrode_matrix['F4']  = [2, 6]\n",
        "electrode_matrix['F8']  = [2, 8]\n",
        "electrode_matrix['FC5']  = [3, 1]\n",
        "electrode_matrix['FC1']  = [3, 3]\n",
        "electrode_matrix['FC2']  = [3, 5]\n",
        "electrode_matrix['FC6']  = [3, 7]\n",
        "electrode_matrix['T7']  = [4, 0]\n",
        "electrode_matrix['C3']  = [4, 2]\n",
        "electrode_matrix['CZ']  = [4, 4]\n",
        "electrode_matrix['C4']  = [4, 6]\n",
        "electrode_matrix['T8']  = [4, 8]\n",
        "electrode_matrix['CP5']  = [5, 1]\n",
        "electrode_matrix['CP1']  = [5, 3]\n",
        "electrode_matrix['CP2']  = [5, 5]\n",
        "electrode_matrix['CP6']  = [5, 7]\n",
        "electrode_matrix['P7']  = [6, 0]\n",
        "electrode_matrix['P3']  = [6, 2]\n",
        "electrode_matrix['PZ']  = [6, 4]\n",
        "electrode_matrix['P4']  = [6, 6]\n",
        "electrode_matrix['P8']  = [6, 8]\n",
        "electrode_matrix['PO3'] = [7, 3]\n",
        "electrode_matrix['PO4'] = [7, 5]\n",
        "electrode_matrix['O1'] = [8, 3]\n",
        "electrode_matrix['OZ'] = [8, 4]\n",
        "electrode_matrix['O2'] = [8, 5]\n",
        "\n",
        "list_electrodes = ['FP1', 'AF3', 'F3', 'F7', 'FC5', 'FC1', 'C3',\t'T7',\t'CP5',\t'CP1',\t'P3',\t'P7',\t'PO3',\t'O1',\t'OZ',\t'PZ',\t'FP2',\t'AF4', 'FZ', 'F4', 'F8', 'FC6',\t'FC2',\t'CZ', 'C4', 'T8', 'CP6',\t'CP2',\t'P4', \t'P8',\t'PO4',\t'O2']\n",
        "data_dir = './data_preprocessed_python'\n",
        "TRAIN_SIZE = 0.9\n",
        "THRESHOLD = 4.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fHodOAszBfB"
      },
      "outputs": [],
      "source": [
        "#  data_samples = []\n",
        "#  labels = []\n",
        "#  interval = 3\n",
        "#  shift = 64\n",
        "#  for sub in range(len(data)): #sub   - человек\n",
        "#       for nvideo in range(NVIDEOS):\n",
        "#             for ntime in range(0, (LEN_RECORD_IN_SECONDS - interval) * 128, shift):\n",
        "#                    temp = data[sub][nvideo, :,  3 * NTIMES_IN_SEC + ntime : (3 + interval) *  NTIMES_IN_SEC + ntime].copy()\n",
        "#                    #data_sample = np.zeros((NCANALS, NTIMES_IN_SEC + 1),  dtype=np.float16)\n",
        "#                    data_sample = np.zeros((NCANALS, NTIMES_IN_SEC + 1))\n",
        "#                    for i in range(NCANALS):\n",
        "#                         freq, data_sample[i] = signal.welch(temp[i], fs=128.0, window='hanning', nperseg=256, noverlap=None, nfft=None, detrend='constant', return_onesided=True, scaling='density', axis=-1)\n",
        "#                    data_samples.append(data_sample)\n",
        "#                    labels.append(labels_bin[sub][nvideo, :])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clyP4skX1oHy"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# with open('data_samples_psd2.pkl', 'wb') as fp:\n",
        "#         pickle.dump(data_samples, fp)\n",
        "#         pickle.dump(labels, fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEKKYsqsuoYv"
      },
      "outputs": [],
      "source": [
        "del data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaa90Z0caR5s"
      },
      "outputs": [],
      "source": [
        "with open('data_samples_psd2.pkl', 'rb') as fp:\n",
        "     data_samples =  pickle.load(fp)\n",
        "     labels = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqaxTpN0YpTe"
      },
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__ (self, data_samples, labels, inds):  #indexes - индексы видео которые вошли в датасет, data - данные labels - метки бинарные\n",
        "       self.data_samples = data_samples\n",
        "       self.labels = labels\n",
        "       self.data_samples = np.array(self.data_samples)[inds]\n",
        "       self.labels = np.array(self.labels)[inds]\n",
        "     \n",
        "\n",
        "    def __len__(self):\n",
        "       result =  len(self.data_samples)\n",
        "       return result\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "       sample = {}   \n",
        "       sample = {\"labels\": self.labels[item],\n",
        "                 \"data\": self.data_samples[item]\n",
        "       }\n",
        "       \n",
        "       if transforms is not None:\n",
        "           for t in transforms:\n",
        "                sample = t(sample)\n",
        "       \n",
        "       return sample\n",
        "                 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Bgkq8a8rwH"
      },
      "source": [
        "delta (0-3 Hz), theta (4-7\n",
        "Hz), low alpha (8-9.5 Hz), high alpha (10.5-12 Hz), alpha (8-\n",
        "12 Hz), low beta (13-16 Hz), mid beta (17-20 Hz), high beta\n",
        "(21-29 Hz), beta (13-29 Hz), and gamma (30-50 Hz)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vWngb8RcqON"
      },
      "outputs": [],
      "source": [
        "# class Compose(object):\n",
        "#     def __init__(self, transforms):\n",
        "#         super(Compose, self).__init__()\n",
        "#         self.transforms = transforms\n",
        "\n",
        "#     def __call__(self, sample):\n",
        "#         for t in self.transforms:\n",
        "#             sample = t(sample)\n",
        "#         return sample\n",
        "import matplotlib \n",
        "from matplotlib import pyplot as plt\n",
        " \n",
        "class to_pcc_matrix_per_freq(object):\n",
        "    def __init__(self):\n",
        "        super(to_pcc_matrix_per_freq, self).__init__()\n",
        "  \n",
        "    def __call__(self, sample: dict):\n",
        "         freq_data = sample['data'].copy()\n",
        "         \n",
        "         freq_ranges_gc = np.array([0, 3, 4, 7, 8, 9.5, 10.5, 12, 8, 12, 13, 16, 17,20, 21, 29, 13, 29, 30, 50])\n",
        "         koeff = 0.5\n",
        "         freq_ranges_ind = ((freq_ranges_gc)/0.5).astype(int)\n",
        "         list_canals = np.arange(NCANALS)\n",
        "         freq_data_new = freq_data\n",
        "         #print(freq_data_new )\n",
        "         freq_data_new = scipy.stats.zscore(freq_data_new)\n",
        "         #print(freq_data_new )\n",
        "         pcc_matrixes = []\n",
        "         for s in range(10):\n",
        "            ind_begin = freq_ranges_ind[s * 2]\n",
        "            ind_end = freq_ranges_ind[s * 2 + 1]\n",
        "            pcc_matrix = np.zeros((27, 27))\n",
        "            for i in range(NCANALS):\n",
        "                pcc_matrix[electrode_matrix[list_electrodes[i]][0] * 3, electrode_matrix[list_electrodes[i]][1] * 3]  = (freq_data_new[i, ind_begin : ind_end ]).sum()#/sum_canals[i]\n",
        "            pcc_matrixes.append(pcc_matrix)\n",
        "            #print(pcc_matrix)\n",
        "         sample['data'] = np.array(pcc_matrixes)\n",
        "         #print(sample['data'])\n",
        "         return(sample)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class to_pcc_matrix(object):\n",
        "#     def __init__(self):\n",
        "#         super(to_pcc_matrix, self).__init__()\n",
        "        \n",
        "\n",
        "#     def __call__(self, sample: dict):\n",
        "#         canal_data = sample['data'].copy()\n",
        "#         list_canals = np.arange(NCANALS)\n",
        "#         random.shuffle(list_canals)\n",
        "#         canal_data_new = [canal_data[list_canals[i]] for i in range(32)]\n",
        "#         pcc_matrix = np.cov(canal_data_new)\n",
        "#         for i in range(NCANALS):\n",
        "#            for j in range(i + 1):\n",
        "#               pcc_matrix[i, j] = pcc_matrix[i, j]/(np.std(canal_data_new[i]) * np.std(canal_data_new[j]))\n",
        "#               pcc_matrix[j, i] = pcc_matrix[i, j]\n",
        "#         sample['data'] = pcc_matrix\n",
        "#         #print(pcc_matrix)\n",
        "#         return(sample)\n",
        "\n",
        "class ToTensor(object):\n",
        "\n",
        "    def __init__(self, ):\n",
        "        super(ToTensor, self).__init__()\n",
        "\n",
        "    def __call__(self, sample: dict):\n",
        "        return {\"labels\": torch.tensor(sample[\"labels\"], dtype=torch.long),\n",
        "                \"data\": torch.tensor(sample[\"data\"], dtype=torch.float32),\n",
        "                } \n",
        "\n",
        "\n",
        "transforms = [to_pcc_matrix_per_freq(),\n",
        "                   ToTensor()]                       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qvGtNlztH8X"
      },
      "outputs": [],
      "source": [
        "def get_conv_module(n_input_channels,n_output_channels):\n",
        "  conv = nn.Conv2d(n_input_channels, n_output_channels, kernel_size = 3, stride=1, padding='same')\n",
        "  maxpool = nn.MaxPool2d(kernel_size = 2)\n",
        "  bn = nn.BatchNorm2d(n_output_channels)\n",
        "  relu = nn.ReLU()\n",
        "  result = torch.nn.Sequential(conv, maxpool, bn, relu)\n",
        "  return(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_2KngC8ruqp"
      },
      "outputs": [],
      "source": [
        "class EmotionNet(torch.nn.Module): \n",
        "   def __init__(self, hcanals, wcanals, tures, ntimes_in_sample):\n",
        "      super().__init__()\n",
        "      n_input_channels = [10, 32, 64, 128]#, 256]\n",
        "      n_output_channels = [32, 64, 128, 256]#, 512]\n",
        "      self.convs = nn.ModuleList([get_conv_module(n_input_channels[i], n_output_channels[i]) for i in range(3)])\n",
        "      self.conv = nn.Conv2d(n_input_channels[3], n_output_channels[3], kernel_size = 3, stride=1, padding='same')\n",
        "      self.flat = nn.Flatten(1, 3)\n",
        "      self.fc = nn.Linear(2304, 2)\n",
        "\n",
        "      \n",
        "   def forward(self, input):\n",
        "      #print(f\"input_shape = {input.shape}\")\n",
        "      #input = input.unsqueeze(3)\n",
        "      #input = input.permute(0, 3, 1, 2)\n",
        "      #input (bs, h=32, w=32, 1)\n",
        "\n",
        "      output = input\n",
        "      for i, conv in enumerate(self.convs):\n",
        "            output = conv(output)\n",
        "            #print(outputs[i])\n",
        "      output = self.conv(output)\n",
        "      output = self.flat(output)\n",
        "      #output (bs, s * 256,  h=9, w=9)\n",
        "      output = self.fc(output)\n",
        "      return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7AXDwTf5Hy7"
      },
      "outputs": [],
      "source": [
        "writer = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRGvG4QaaYl6"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, criterion, optimizer, device, val_dataloader, val_f1_min,  description, type_emotion, batch = None, writer = None):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    inputs = []\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "   \n",
        "    #lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)#, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
        "    for i , batch in enumerate(loader):#, total=len(loader), desc=\"training...\", position=0 , leave = True)):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            src  = batch['data'].to(device)\n",
        "            trg = batch['labels'][:, type_emotion]\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            loss = criterion(levels_pred, trg) \n",
        "            train_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if writer:\n",
        "                writer.add_scalar(f'{description}/training loss per batch',\n",
        "                                  loss.item(),\n",
        "                                  i)\n",
        "\n",
        "            # if (i % 100 == 0):\n",
        "            #     acc, f1 = (calculate_predictions(model, val_dataloader))\n",
        "            #     if (f1 > val_f1_min):\n",
        "            #           val_f1_min      = f1\n",
        "            #           torch.save({'model_state_dict': model.state_dict(),    'optimizer_state_dict': optimizer.state_dict(),}, os.path.join(args.output_dir, f\"val.tgz\"))\n",
        "            #break\n",
        "    return np.mean(train_loss), val_f1_min\n",
        "\n",
        "def evaluate(model, loader, criterion, device, writer, description, type_emotion):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "  \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for s, batch in enumerate(loader):#, total=len(loader), desc=\"validating...\", position=0 , leave = True)):\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels'][:, type_emotion]\n",
        "\n",
        "\n",
        "\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            #print(levels_pred.shape)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            loss = criterion(levels_pred, trg) \n",
        "            epoch_loss += loss.item() \n",
        "\n",
        "\n",
        "            if writer:\n",
        "                writer.add_scalar(f'{description}/val loss per batch',\n",
        "                                  loss.item(),\n",
        "                                  s)\n",
        "        \n",
        "    return epoch_loss / s\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
        "\n",
        "def calculate_predictions(model, loader, type_emotion, show):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "    real = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(loader):#, total=len(loader), desc=\"predicting...\", position=0 , leave = True)):\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels'][:, type_emotion]\n",
        "         \n",
        "\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            #print(levels_pred.shape)\n",
        "            trg_pred = levels_pred.argmax(1)\n",
        "            \n",
        "            real.extend(trg)\n",
        "            pred.extend(trg_pred) \n",
        "\n",
        "        if show:    \n",
        "            print(accuracy_score(real, pred)) \n",
        "            print(confusion_matrix(real, pred))  \n",
        "            print(classification_report(real, pred))  \n",
        "        f1 = ((f1_score(real, pred, average = 'binary', pos_label = 0))  + (f1_score(real, pred, average = 'binary', pos_label = 1)))/2\n",
        "        return (accuracy_score(real, pred)) , f1\n",
        "        #plt.hist(real)\n",
        "def train_loop(description, type_emotion, n_epochs = 10):\n",
        "    #args.epochs = 10\n",
        "    #criterion =  fnn.mse_loss\n",
        "    train_loss_min = 10000\n",
        "    val_f1_min = -10000\n",
        "\n",
        "    #batch = next(iter(train_dataloader))\n",
        "    for epoch in range(n_epochs):\n",
        "          #logger.info(f\"Starting epoch {epoch + 1}/{args.epochs}.\")\n",
        "    \n",
        "          train_loss, val_f1_min  = train(model, train_dataloader, criterion, optimizer ,device, val_dataloader, val_f1_min ,  description , type_emotion, None,  writer)\n",
        "          #if epoch % 500 == 0:\n",
        "          if writer:\n",
        "                writer.add_scalar(f\"{description}/training loss per epoch\",\n",
        "                                        train_loss,\n",
        "                                        epoch)\n",
        "          #print(train_loss)\n",
        "\n",
        "          if (train_loss < train_loss_min):\n",
        "                 train_loss_min      = train_loss\n",
        "                 torch.save({\n",
        "                         'model_state_dict': model.state_dict(),\n",
        "                         'optimizer_state_dict': optimizer.state_dict(),\n",
        "                       },\n",
        "                       os.path.join(args.output_dir, \"train.tgz\")\n",
        "            )  \n",
        "\n",
        "          #val_loss = evaluate(model, val_dataloader, criterion, device,  writer, description )\n",
        "          # #break\n",
        "          #print(val_loss)\n",
        "          #if writer:\n",
        "          #      writer.add_scalar(f\"{description}/val loss per epoch\",\n",
        "          #                        val_loss,\n",
        "          #                        epoch)\n",
        "          acc, f1 = (calculate_predictions(model, val_dataloader, type_emotion, False))\n",
        "          print(acc, f1)\n",
        "          if (acc > val_f1_min):\n",
        "                       val_f1_min      = acc\n",
        "                       torch.save({'model_state_dict': model.state_dict(),    'optimizer_state_dict': optimizer.state_dict(),}, os.path.join(args.output_dir, f\"val_{description}.tgz\")) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1I4ROvi503x",
        "outputId": "729a2058-fbd7-407b-df36-12438164a2c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EmotionNet(\n",
              "  (convs): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU()\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU()\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (3): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (flat): Flatten(start_dim=1, end_dim=3)\n",
              "  (fc): Linear(in_features=2304, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "def get_model():\n",
        "  model = EmotionNet(HCANALS, WCANALS, NFEATURES, NTIMES_IN_SAMPLE).to(device)\n",
        "  return model\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_model()\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am5PMLd4cqq7"
      },
      "source": [
        "#Разбиение с ликом"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o22lXEwPC1K_"
      },
      "outputs": [],
      "source": [
        "type_emotion  = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2CYyhkSbWiN"
      },
      "outputs": [],
      "source": [
        "sample = ( train_dataset.__getitem__(20) )\n",
        "print(sample['data'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU_R_oC7G7zr",
        "outputId": "39a89cde-f84a-4957-ef78-0c86146a9287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold = 0\n",
            "0.6955523574561403 0.6293091481796738\n",
            "0.7086074561403509 0.6721159500677063\n",
            "0.7248834978070176 0.681517806229814\n",
            "0.7430098684210527 0.712450113944007\n",
            "0.7426329495614035 0.7121218620288612\n",
            "0.7503769188596491 0.7273633557116426\n",
            "0.7508223684210527 0.7316958632044008\n",
            "0.7664473684210527 0.738987060371418\n",
            "0.7660361842105263 0.7265826211255043\n",
            "0.7693256578947368 0.7393015525771831\n",
            "0.7747053179824561 0.7490882745480334\n",
            "0.7625068530701754 0.7498726819438277\n",
            "0.7760759320175439 0.7454214649937395\n",
            "0.784094024122807 0.759238352338075\n",
            "0.7894394188596491 0.7657938854503679\n",
            "0.780187774122807 0.7632359275178187\n",
            "0.7878974780701754 0.7650762975197533\n",
            "0.7912554824561403 0.7714553670632502\n",
            "0.7835800438596491 0.7693537025665274\n",
            "0.7927288925438597 0.7731835912097231\n",
            "0.7927288925438597\n",
            "[[ 7284  3500]\n",
            " [ 2549 15851]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.68      0.71     10784\n",
            "           1       0.82      0.86      0.84     18400\n",
            "\n",
            "    accuracy                           0.79     29184\n",
            "   macro avg       0.78      0.77      0.77     29184\n",
            "weighted avg       0.79      0.79      0.79     29184\n",
            "\n",
            "************f1 = 0.7731835912097231 acc = 0.7927288925438597********************\n",
            "fold = 1\n",
            "0.6811951754385965 0.6432134005634615\n",
            "0.7078536184210527 0.6583597845926111\n",
            "0.7163856907894737 0.6716612929348917\n",
            "0.7354029605263158 0.6874621401633623\n",
            "0.7351973684210527 0.6915148688267282\n",
            "0.7436951754385965 0.7249116936528686\n",
            "0.7416735197368421 0.7268574048154317\n",
            "0.7592516447368421 0.7338747282325815\n",
            "0.7621641995614035 0.7366302943930373\n",
            "0.7635348135964912 0.7358827808810768\n",
            "0.7658648574561403 0.7313465773359621\n",
            "0.7755619517543859 0.7502065499766213\n",
            "0.7716214364035088 0.7548043461970164\n",
            "0.7765556469298246 0.762919949201954\n",
            "0.7864240679824561 0.7614604994568556\n",
            "0.7856702302631579 0.759304304669413\n",
            "0.7877604166666666 0.7594876177945986\n",
            "0.7797765899122807 0.7531367050977772\n",
            "0.7930372807017544 0.7675953744202519\n",
            "0.7866981907894737 0.7631676066666235\n",
            "0.7930372807017544\n",
            "[[ 6744  4013]\n",
            " [ 2027 16400]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.63      0.69     10757\n",
            "           1       0.80      0.89      0.84     18427\n",
            "\n",
            "    accuracy                           0.79     29184\n",
            "   macro avg       0.79      0.76      0.77     29184\n",
            "weighted avg       0.79      0.79      0.79     29184\n",
            "\n",
            "************f1 = 0.7675953744202519 acc = 0.7930372807017544********************\n",
            "fold = 2\n",
            "0.676672149122807 0.6266324492446695\n",
            "0.7035361842105263 0.6565135916731983\n",
            "0.7122395833333334 0.6876004969284597\n",
            "0.7276932565789473 0.6898263224120071\n",
            "0.7306058114035088 0.706246469625645\n",
            "0.7421532346491229 0.7017016310069295\n",
            "0.7421189692982456 0.7191499972111762\n",
            "0.7545915570175439 0.7339662336787582\n",
            "0.7557223135964912 0.7237021442786225\n",
            "0.7541118421052632 0.7195429527006543\n",
            "0.7610677083333334 0.7348874424428189\n",
            "0.749109100877193 0.7399880924090385\n",
            "0.7742256030701754 0.744549926080524\n",
            "0.7708333333333334 0.7453384202345625\n",
            "0.7779947916666666 0.7517378035980944\n",
            "0.7791255482456141 0.751808948282306\n",
            "0.7791940789473685 0.7629574452126187\n",
            "0.7795367324561403 0.759910736899302\n",
            "0.7821751644736842 0.7591525679997693\n",
            "0.7762472587719298 0.7639543449900138\n",
            "0.7821751644736842\n",
            "[[ 6902  3834]\n",
            " [ 2523 15925]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.64      0.68     10736\n",
            "           1       0.81      0.86      0.83     18448\n",
            "\n",
            "    accuracy                           0.78     29184\n",
            "   macro avg       0.77      0.75      0.76     29184\n",
            "weighted avg       0.78      0.78      0.78     29184\n",
            "\n",
            "************f1 = 0.7591525679997693 acc = 0.7821751644736842********************\n",
            "fold = 3\n",
            "0.6929481907894737 0.6343976898919941\n",
            "0.7087787828947368 0.6466547667864386\n",
            "0.7151864035087719 0.6898583026226102\n",
            "0.7338952850877193 0.7021025863663231\n",
            "0.7384525767543859 0.7182709948898256\n",
            "0.739172149122807 0.7197109701470871\n",
            "0.7435923793859649 0.7270283952121019\n",
            "0.7587719298245614 0.7315066613047286\n",
            "0.7638089364035088 0.7436335865119308\n",
            "0.7573670504385965 0.7180588948654197\n",
            "0.7706962719298246 0.7464307242615731\n",
            "0.7629180372807017 0.7417077827270857\n",
            "0.7690172697368421 0.7474025346065748\n",
            "0.7747053179824561 0.7516441472921126\n",
            "0.7815583881578947 0.7580036018270024\n",
            "0.7826548793859649 0.7578590041986677\n",
            "0.7846422697368421 0.7682166060099729\n",
            "0.7868352521929824 0.7710501116146329\n",
            "0.7835115131578947 0.7706933686771791\n",
            "0.7907757675438597 0.7738591793415888\n",
            "0.7907757675438597\n",
            "[[ 7548  3115]\n",
            " [ 2991 15530]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.71      0.71     10663\n",
            "           1       0.83      0.84      0.84     18521\n",
            "\n",
            "    accuracy                           0.79     29184\n",
            "   macro avg       0.77      0.77      0.77     29184\n",
            "weighted avg       0.79      0.79      0.79     29184\n",
            "\n",
            "************f1 = 0.7738591793415888 acc = 0.7907757675438597********************\n",
            "fold = 4\n",
            "0.6833538925438597 0.5769340587181973\n",
            "0.6977453399122807 0.633446459869657\n",
            "0.6997669956140351 0.6806976798594004\n",
            "0.7089158442982456 0.6946264657755239\n",
            "0.7208059210526315 0.6967734485143647\n",
            "0.7354029605263158 0.7152161472724003\n",
            "0.7498972039473685 0.7256197752884942\n",
            "0.7555167214912281 0.7215644661065606\n",
            "0.7510964912280702 0.7399810694683298\n",
            "0.7555852521929824 0.7416801639030302\n",
            "0.7626439144736842 0.7368872943440063\n",
            "0.7571957236842105 0.7320640088922932\n",
            "0.764905427631579 0.7417836614163107\n",
            "0.7792626096491229 0.7574270547035662\n",
            "0.7812842653508771 0.7561290738181785\n",
            "0.7837171052631579 0.7656751204000745\n",
            "0.7718955592105263 0.7554549985121903\n",
            "0.7882401315789473 0.7703857605773331\n",
            "0.780530427631579 0.7630104553210784\n",
            "0.7863555372807017 0.7637083047863709\n",
            "0.7882401315789473\n",
            "[[ 7433  3435]\n",
            " [ 2745 15571]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.68      0.71     10868\n",
            "           1       0.82      0.85      0.83     18316\n",
            "\n",
            "    accuracy                           0.79     29184\n",
            "   macro avg       0.77      0.77      0.77     29184\n",
            "weighted avg       0.79      0.79      0.79     29184\n",
            "\n",
            "************f1 = 0.7703857605773331 acc = 0.7882401315789473********************\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold , KFold\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "interval = 3\n",
        "shift = 1\n",
        "koeff = 0.8\n",
        "import pandas as pd    \n",
        "type_emotion  = 0\n",
        "# #inds = np.arange(2280)\n",
        "#inds = np.arange(72960)\n",
        "args.batch_size = 256\n",
        "\n",
        "# #inds = np.arange(12800)\n",
        "# random.shuffle(inds)\n",
        "acc_all = []\n",
        "f1_all = []\n",
        "inds = np.arange(len(data_samples))\n",
        "kf = KFold(n_splits=5, shuffle = True)\n",
        "for fold,  (inds_train, inds_test) in enumerate(kf.split(inds)):\n",
        "    print(f\"fold = {fold}\")\n",
        "    #if fold == 0:\n",
        "      #continue       \n",
        "    #transforms_random = RandomAugmentation([add_noise, reset_part_in_freq, reset_part_in_time, None], 0.2)\n",
        "    #transforms = [RandomAugmentation([add_noise(), reset_part_in_freq(0.2), reset_part_in_time(0.2), None], 0.2), to_head_matrix(),ToTensor()]   \n",
        "    transforms = [to_pcc_matrix_per_freq(),   ToTensor()]       \n",
        "    #transforms = [RandomAugmentation([add_noise(), None], 0.5), to_head_matrix(),ToTensor()]   \n",
        "    #transforms = [RandomAugmentation([reset_part_in_time(0.4), None], 0.1), to_head_matrix(),ToTensor()]   \n",
        "\n",
        "    train_dataset = EmotionDataset(data_samples, labels, inds_train)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                            pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "    val_dataset = EmotionDataset(data_samples, labels, inds_test)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                              pin_memory=True, shuffle=False, drop_last=False)\n",
        "    #break\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model()\n",
        "    model.apply(initialize_weights)\n",
        "    criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=3e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "    description = f'psd_model_{fold}'\n",
        "    train_loop(description, 0, 20)\n",
        "    model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "    model.load_state_dict(model_state['model_state_dict'])\n",
        "    acc, f1 = calculate_predictions(model, val_dataloader, type_emotion, True)\n",
        "    print(f\"************f1 = {f1} acc = {acc}********************\")\n",
        "    acc_all.append(acc)\n",
        "    f1_all.append(f1)\n",
        "        \n",
        "    pd.DataFrame(f1_all).to_csv(\"f1_val_psd_leak_result.csv\")\n",
        "    pd.DataFrame(acc_all).to_csv(\"acc_val_psd_leak_result.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKj9KeXmgCxO"
      },
      "outputs": [],
      "source": [
        "# with open('train_dataset.pkl', 'wb') as fp:\n",
        "#        pickle.dump(train_dataset, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TWYCiGngcgD"
      },
      "outputs": [],
      "source": [
        "# with open('val_dataset.pkl', 'wb') as fp:\n",
        "#        pickle.dump(val_dataset, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43CamcII_sDG"
      },
      "outputs": [],
      "source": [
        "# with open('train_dataset.pkl', 'rb') as fp:\n",
        "#        train_dataset = pickle.load(fp)\n",
        "# with open('val_dataset.pkl', 'rb') as fp:\n",
        "#         val_dataset = pickle.load(fp)\n",
        "\n",
        "\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "#                                 pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "# val_dataset = EmotionDataset(data, labels_bin, interval, shift, inds, 'test')\n",
        "#val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "#                                pin_memory=True, shuffle=False, drop_last=False)        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSrCAGHUf2Er"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "#optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "# print(train_dataset.cnt)\n",
        "# print(val_dataset.cnt)\n",
        "# print(files[ind_train])\n",
        "# print(files[ind_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33X5NKTqkRGt"
      },
      "outputs": [],
      "source": [
        "val_loss = evaluate(model, val_dataloader, criterion, device)\n",
        "# #break\n",
        "print(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "8bface7940da4f718cab778203dc4f3a",
            "3875c91fd8f44f959d4e1f5e083d7bdc",
            "84d38657d8a84677ba5ad16d1aeb7ea8",
            "76a7fa635d0240a9bd47cacf705a7bdc",
            "bd742ed1c6444fd69acb67ba9e1b2e6d",
            "14b6fbc2cb8b4e5ea32326279daf1862",
            "5301fa5b192d43c4a273f80084074499",
            "e2869148519345f786e5f89c7b00680a",
            "827cf12661a34e48beac4c056783fd08",
            "8863225c61de434391e23578de9cd833",
            "027c5fbe66f9421e823efbb9365510d5"
          ]
        },
        "id": "DO8LGRb6tN4b",
        "outputId": "4a369646-0727-47cf-c21b-e92b565cac8f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bface7940da4f718cab778203dc4f3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "predicting...:   0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f19eb5c80e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7451343201754386\n",
            "[[3232 2156]\n",
            " [1563 7641]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.60      0.63      5388\n",
            "           1       0.78      0.83      0.80      9204\n",
            "\n",
            "    accuracy                           0.75     14592\n",
            "   macro avg       0.73      0.72      0.72     14592\n",
            "weighted avg       0.74      0.75      0.74     14592\n",
            "\n",
            "(0.7451343201754386, 0.7195284609627088)\n"
          ]
        }
      ],
      "source": [
        "model_state  = torch.load(os.path.join(\"/content/drive/MyDrive/MADE/Project/CNN_models/\", f\"val_5.tgz\"))\n",
        "        #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "model.load_state_dict(model_state['model_state_dict'])\n",
        "print(calculate_predictions(model, val_dataloader))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "MADE_PSD_leak_beta1.ipynb",
      "provenance": [],
      "mount_file_id": "1Oa9jKg0kyChnxhft2Ly-U6HPjo0Hde7i",
      "authorship_tag": "ABX9TyMWbPSSfIVqMADUzFMmiAmN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027c5fbe66f9421e823efbb9365510d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14b6fbc2cb8b4e5ea32326279daf1862": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3875c91fd8f44f959d4e1f5e083d7bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14b6fbc2cb8b4e5ea32326279daf1862",
            "placeholder": "​",
            "style": "IPY_MODEL_5301fa5b192d43c4a273f80084074499",
            "value": "predicting...: 100%"
          }
        },
        "5301fa5b192d43c4a273f80084074499": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76a7fa635d0240a9bd47cacf705a7bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8863225c61de434391e23578de9cd833",
            "placeholder": "​",
            "style": "IPY_MODEL_027c5fbe66f9421e823efbb9365510d5",
            "value": " 57/57 [00:18&lt;00:00,  3.29it/s]"
          }
        },
        "827cf12661a34e48beac4c056783fd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84d38657d8a84677ba5ad16d1aeb7ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2869148519345f786e5f89c7b00680a",
            "max": 57,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_827cf12661a34e48beac4c056783fd08",
            "value": 57
          }
        },
        "8863225c61de434391e23578de9cd833": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bface7940da4f718cab778203dc4f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3875c91fd8f44f959d4e1f5e083d7bdc",
              "IPY_MODEL_84d38657d8a84677ba5ad16d1aeb7ea8",
              "IPY_MODEL_76a7fa635d0240a9bd47cacf705a7bdc"
            ],
            "layout": "IPY_MODEL_bd742ed1c6444fd69acb67ba9e1b2e6d"
          }
        },
        "bd742ed1c6444fd69acb67ba9e1b2e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2869148519345f786e5f89c7b00680a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}