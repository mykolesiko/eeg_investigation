{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recognition_rnn_psv_many.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1tBdHPAvQsSEp-Rs0n9SpK_L6RlDFwRVo",
      "authorship_tag": "ABX9TyNcRCD2aGiywQ0rrtBvHfrl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mykolesiko/eeg_investigation/blob/master/recognition_rnn_psv_many.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g2FiRDliSpk"
      },
      "source": [
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fecNu3lxwOP"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA0MKeOZaZ3X"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from  scipy import stats\n",
        "import scipy\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qz6C77EuyqU"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8i6x4kaxXah"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/MADE/Project/deap\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDAwJngRhtW6"
      },
      "source": [
        "class Args:\n",
        "  def __init__(self): #(data_path, epoch, batch_siz, image_size, learning_rate, weight_deca, learning_rate, learning_rate_gamma, weight_bce, load, output_dir)\n",
        "    self.data_path = \"/content/drive/MyDrive/MADE/semester2/CV/contest02/data/\"\n",
        "    self.epochs = 2\n",
        "    self.batch_size = 100\n",
        "    self.lr= 3e-4\n",
        "    self.weight_decay= 1e-6\n",
        "    self.learning_rate=None\n",
        "    self.learning_rate_gamma=None\n",
        "    self.weight_bce=1\n",
        "    self.load=None\n",
        "    self.output_dir=\"/content/drive/MyDrive/MADE/Project/lstm_models/\"\n",
        "    self.data_dir =\"./data_preprocessed_python/\"# \"/content/drive/MyDrive/MADE/Project/train/physionet.org/\"\n",
        "args = Args()   "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8y64YC0xx98"
      },
      "source": [
        "class EmotionNet(torch.nn.Module): \n",
        "   def __init__(self, input_size, hidden_size1, hidden_size2, koeff, len_seq):\n",
        "       super().__init__()\n",
        "       self.fc0 = nn.Linear(input_size, input_size)\n",
        "       self.relu = nn.ReLU()\n",
        "       self.rnn1 = nn.LSTM(input_size, hidden_size1, bidirectional = False, batch_first = True)\n",
        "       self.drops = nn.ModuleList([nn.Dropout(koeff)] * len_seq)\n",
        "       self.rnn2 = nn.LSTM(hidden_size1, hidden_size2, bidirectional = False, batch_first = True)\n",
        "       input_linear_size = hidden_size2\n",
        "       self.fc = nn.Linear(input_linear_size, 2)\n",
        "   def forward(self, src):\n",
        "      #src = [batch size, seq_len, input_size\n",
        "      #src = self.fc0(src)\n",
        "      #src = self.relu(src)\n",
        "      #print(src.shape)\n",
        "      encoder_outputs1, (hidden, c) = self.rnn1(src)  \n",
        "\n",
        "      outputs1 = []\n",
        "      for i, drop in enumerate(self.drops):\n",
        "             temp = drop(encoder_outputs1[:, i, :]).unsqueeze(0)\n",
        "             #print(temp.shape)\n",
        "             outputs1.append(temp)\n",
        "      outputs1 = torch.vstack(outputs1).permute(1, 0, 2)       \n",
        "      #print(outputs1.shape)\n",
        "      encoder_outputs2, (hidden, c) = self.rnn2(outputs1)  \n",
        "      #print(hidden.shape)\n",
        "      result = self.fc(hidden)\n",
        "      #print(result.shape)\n",
        "      return (result.squeeze(0))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gle0eN98yTz0"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53e0zIUWljhm"
      },
      "source": [
        "INPUT_SIZE = 1 * 128 * 32\n",
        "HIDDEN_SIZE_1 = 64\n",
        "HIDDEN_SIZE_2 = 32\n",
        "DROPOUT_COEFF = 0.5\n",
        "LEN_SEQ = 10\n",
        "def get_model(INPUT_SIZE, HIDDEN_SIZE_1, HIDDEN_SIZE_2, DROPOUT_COEFF, LEN_SEQ):\n",
        "  model = EmotionNet(INPUT_SIZE, HIDDEN_SIZE_1, HIDDEN_SIZE_2, DROPOUT_COEFF, LEN_SEQ).to(device)\n",
        "  return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBgj1BjIhlnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96cedc62-f906-4a2c-e791-6a5627ea6ff6"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_model(INPUT_SIZE, HIDDEN_SIZE_1, HIDDEN_SIZE_2, DROPOUT_COEFF, LEN_SEQ)\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EmotionNet(\n",
              "  (fc0): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (rnn1): LSTM(4096, 64, batch_first=True)\n",
              "  (drops): ModuleList(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Dropout(p=0.5, inplace=False)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Dropout(p=0.5, inplace=False)\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Dropout(p=0.5, inplace=False)\n",
              "    (9): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (rnn2): LSTM(64, 32, batch_first=True)\n",
              "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57bvhvJpptRb"
      },
      "source": [
        "import glob\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "import scipy\n",
        "from scipy import fft, ifft\n",
        "\n",
        "\n",
        "# class RandomAugmentation(object):\n",
        "#     def __init__(self, augmenters, probability):\n",
        "#         self._augmenters = augmenters\n",
        "#         self.probability = probability\n",
        "\n",
        "#     def __call__(self, sample):\n",
        "#         augmenter = random.choice(self._augmenters)\n",
        "#         #print(augmenter)\n",
        "#         if (augmenter):\n",
        "#             if (np.random.random() < self.probability):\n",
        "#                return augmenter(sample)\n",
        "#         return(sample)  \n",
        "\n",
        "\n",
        "# class add_noise(object):\n",
        "#     def __init__(self, ):\n",
        "#         super(add_noise, self).__init__()\n",
        "\n",
        "#     def __call__(self, sample: dict):\n",
        "#         white_noise = np.random.normal(loc=0.0, scale=1.0, size=sample['data'].shape)\n",
        "#         sample['data'] = sample['data'] + white_noise\n",
        "#         return sample\n",
        "\n",
        "\n",
        "# class reset_part_in_time(object):\n",
        "#     def __init__(self, percentage):\n",
        "#         super(reset_part_in_time, self).__init__()\n",
        "#         self.percentage = percentage\n",
        "\n",
        "#     def __call__(self, sample: dict):\n",
        "#         len_data = len(sample['data'][0])\n",
        "#         interval_to_reset = int(len_data * self.percentage)\n",
        "#         for ncanal in range(NVIDEOS):\n",
        "#             begin = random.randint(0, len_data - interval_to_reset)\n",
        "#             sample['data'][ncanal, begin : begin + interval_to_reset] = 0\n",
        "\n",
        "#         return sample\n",
        "\n",
        "\n",
        "# class reset_part_in_freq(object):\n",
        "#     def __init__(self, percentage):\n",
        "#         super(reset_part_in_freq, self).__init__()\n",
        "#         self.percentage = percentage\n",
        "\n",
        "#     def __call__(self, sample: dict):\n",
        "#         len_data = len(sample['data'][0])\n",
        "#         interval_to_reset = int(len_data * self.percentage)\n",
        "#         power_freq_data = fft(sample['data'])\n",
        "#         for ncanal in range(NVIDEOS):\n",
        "#             begin = random.randint(0, len_data - interval_to_reset)\n",
        "#             power_freq_data[ncanal, begin : begin + interval_to_reset] = 0\n",
        "#         sample['data'] = ifft(power_freq_data)\n",
        "#         return sample\n",
        "NSUBJECTS = 32\n",
        "NVIDEOS = 40\n",
        "NCANALS = 32\n",
        "       \n",
        "\n",
        "# class flatten(object):\n",
        "#     def __init__(self):\n",
        "#         super(flatten, self).__init__()\n",
        "        \n",
        "\n",
        "#     def __call__(self, sample: dict):\n",
        "#         input = sample['data'].copy()\n",
        "#         len_seq = len(input)\n",
        "#         segments = np.array(input).reshape(len_seq, -1)\n",
        "#         sample['data'] = segments\n",
        "#         #print(sample)\n",
        "#         return sample     \n",
        "\n",
        "class to_psd(object):\n",
        "       def __init__(self):\n",
        "           super(to_psd, self).__init__()\n",
        "\n",
        "       def __call__(self, sample: dict):\n",
        "\n",
        "         input = sample['data'].copy()\n",
        "         len_seq = len(input)\n",
        "         input_new = []\n",
        "         for time in range(len_seq):\n",
        "              features = []\n",
        "              freq_data = input[time]\n",
        "              freq_ranges_gc = np.array([0, 3, 4, 7, 8, 9.5, 10.5, 12, 8, 12, 13, 16, 17,20, 21, 29, 13, 29, 30, 50])\n",
        "              koeff = 0.5\n",
        "              freq_ranges_ind = ((freq_ranges_gc)/0.5).astype(int)\n",
        "              freq_data_new = freq_data#np.sqrt(np.array(freq_data).real ** 2 + np.array(freq_data).imag ** 2)\n",
        "              #freq_data_new = scipy.stats.zscore(freq_data_new)\n",
        "              for i in range(NCANALS):\n",
        "                  #pcc_matrix = np.zeros((27, 27))\n",
        "                      for s in range(10):    \n",
        "                           ind_begin = freq_ranges_ind[s * 2]\n",
        "                           ind_end = freq_ranges_ind[s * 2 + 1]\n",
        "                           features.append(freq_data_new[i, ind_begin : ind_end].sum())\n",
        "                           for j in range(s):\n",
        "                             features.append(features[s]/features[j])\n",
        "                        \n",
        "              input_new.append(features)\n",
        "         #sample['data'] = scipy.stats.zscore(np.array(input_new))\n",
        "         sample['data'] = (np.array(input_new))\n",
        "         return(sample)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "class ToTensor(object):\n",
        "\n",
        "    def __init__(self, ):\n",
        "        super(ToTensor, self).__init__()\n",
        "\n",
        "    def __call__(self, sample: dict):\n",
        "        return {\"labels\": torch.tensor(sample[\"labels\"], dtype=torch.long),\n",
        "                \"data\": torch.tensor(sample[\"data\"], dtype=torch.float32),\n",
        "                } \n",
        "\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "   def __init__ (self, data, labels, transforms, inds, interval = 1, shift = 1, len_seq = 10):\n",
        "       self.data = data\n",
        "       self.labels = labels\n",
        "       self.transforms = transforms\n",
        "       self.interval = interval\n",
        "       self.len_seq = len_seq\n",
        "       self.cnt = [Counter(), Counter(), Counter(),Counter()]\n",
        "       self.indexes = []\n",
        "       for sub in range(len(data)):\n",
        "            for nvideo in inds[sub]:\n",
        "               for nsec in range(3, LEN_RECORD + 3, shift):\n",
        "                  if ((nsec + len_seq * interval) > LEN_RECORD + 3):\n",
        "                    break\n",
        "                  self.indexes.append((sub, nvideo, nsec))\n",
        "\n",
        "            \n",
        "   def __len__(self):\n",
        "        result =  len(self.indexes)\n",
        "        return result\n",
        "\n",
        "   def __getitem__(self, item):         \n",
        "       sample = {}   \n",
        "       (sub, nvideo, nsec) = self.indexes[item]\n",
        "       sample['labels'] = self.labels[sub][nvideo, :]\n",
        "       sample['data'] = []\n",
        "       nsec_it = nsec\n",
        "       for i in range(self.len_seq):\n",
        "          sample['data'].append(self.data[sub][nvideo, :32, nsec_it * 128 : (nsec_it + self.interval) * 128])\n",
        "          nsec_it += self.interval\n",
        "       \n",
        "       if self.transforms is not None:\n",
        "           for t in self.transforms:\n",
        "                sample = t(sample)\n",
        "       #print(sample)         \n",
        "       return sample\n",
        "       "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syV3PwNKwSfd"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXFYsRzpvPpS"
      },
      "source": [
        "import glob\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "data_dir = './data_preprocessed_python'\n",
        "files = glob.glob(os.path.join(data_dir, \"*.dat\"))\n",
        "data_raw = []\n",
        "for file_data in files:\n",
        "    raw_data = pickle.load(open(file_data, 'rb'), encoding='latin1')\n",
        "    data.append(raw_data['data'])\n",
        "    labels.append(raw_data['labels'])\n",
        "    #break"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abJs3AICQjlL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1lXH6bikegN"
      },
      "source": [
        "# import glob\n",
        "# import pickle\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# data = []\n",
        "# labels = []\n",
        "# data_dir = './data_preprocessed_python'\n",
        "# files = glob.glob(os.path.join(data_dir, \"*.dat\"))\n",
        "# data_raw = []\n",
        "# for file_data in files:\n",
        "#     raw_data = pickle.load(open(file_data, 'rb'), encoding='latin1')\n",
        "#     data.append(raw_data['data'])\n",
        "#     labels.append(raw_data['labels'])\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcvFIVrD49gA"
      },
      "source": [
        "# import glob\n",
        "# import pickle\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# data = []\n",
        "# labels = []\n",
        "# data_dir = './data_preprocessed_python'\n",
        "# files = glob.glob(os.path.join(data_dir, \"*.npy\"))\n",
        "# data_raw = []\n",
        "# for file_data in files:\n",
        "#     raw_data = np.load(open(file_data, 'rb'))\n",
        "#     data.append(raw_data)\n",
        "    #labels.append(raw_data['labels'])\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i86-P8XuXYiD"
      },
      "source": [
        "# import glob\n",
        "# import pickle\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# data_orig = []\n",
        "# labels = []\n",
        "# data_dir = './data_preprocessed_python'\n",
        "# files = glob.glob(os.path.join(data_dir, \"*orig.npy\"))\n",
        "# data_raw = []\n",
        "# for file_data in files:\n",
        "#     raw_data = np.load(open(file_data, 'rb'))\n",
        "#     data_orig.append(raw_data)\n",
        "    #labels.append(raw_data['labels'])\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcDjKd2RXd3P"
      },
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# for i in range(1):\n",
        "#   fig, ax = plt.subplots(1, 1, figsize = (70, 5))\n",
        "#   ax.plot(data_orig[0][i, 0, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai-H0BouSWxB"
      },
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# for i in range(1):\n",
        "#   fig, ax = plt.subplots(1, 1, figsize = (70, 5))\n",
        "#   ax.plot(data[1][i, 0, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IW74tnxRtOE"
      },
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# for i in range(40):\n",
        "#   fig, ax = plt.subplots(1, 1, figsize = (70, 5))\n",
        "#   ax.plot(data_prep[2][i, 0, :])\n",
        "#   #plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbbJ-Dp5zC-h"
      },
      "source": [
        "# def normalize_data1():\n",
        "#   for sub in range(NSUBJECTS):\n",
        "#     for nvideo in range(NVIDEOS):\n",
        "#         for ncanal in range(NCANALS):\n",
        "#              std = np.std(data[sub][nvideo, ncanal, :])\n",
        "#              data[sub][nvideo, ncanal, :] = np.clip(data[sub][nvideo, ncanal, :], -3 * std, 3 * std)\n",
        "#     scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "#     data[sub][nvideo, :, :] = scaler.fit_transform(data[sub][nvideo,:, :].reshape(-1, 1)).reshape(1, 40, -1)         \n",
        "\n",
        "# def normalize_data2():\n",
        "#   for sub in range(NSUBJECTS):\n",
        "#     for nvideo in range(NVIDEOS):\n",
        "#         for ncanal in range(NCANALS):\n",
        "#              std = np.std(data[sub][nvideo, ncanal, :])\n",
        "#              data[sub][nvideo, ncanal, :] = np.clip(data[sub][nvideo, ncanal, :], -3 * std, 3 * std)\n",
        "#              scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "#              data[sub][nvideo, ncanal, :] = scaler.fit_transform(data[sub][nvideo, ncanal, :].reshape(-1, 1)).reshape(1, -1)\n",
        "\n",
        "\n",
        "# def normalize_data3():\n",
        "#   for sub in range(NSUBJECTS):\n",
        "#     for nvideo in range(NVIDEOS):\n",
        "#         for ncanal in range(NCANALS):\n",
        "#              #std = np.std(data[sub][nvideo, ncanal, :])\n",
        "#              #data[sub][nvideo, ncanal, :] = np.clip(data[sub][nvideo, ncanal, :], -3 * std, 3 * std)\n",
        "#              scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "#              data[sub][nvideo, ncanal, :] = scaler.fit_transform(data[sub][nvideo, ncanal, :].reshape(-1, 1)).reshape(1, -1) \n",
        "#              data[sub][nvideo, ncanal, :] = data[sub][nvideo, ncanal, :] - data[sub][nvideo, ncanal, :].mean()\n",
        "\n",
        "# def normalize_data4():\n",
        "#   for sub in range(len(data)):\n",
        "#      scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "#      data[sub][:, :, :] = scaler.fit_transform(data[sub][:, :, :].reshape(-1, 1)).reshape(40, 32, -1)             \n",
        "\n",
        "# def zscore_data():\n",
        "#   for sub in range(NSUBJECTS):\n",
        "#     for nvideo in range(NVIDEOS):\n",
        "#         for ncanal in range(NCANALS):\n",
        "#              data[sub][nvideo, ncanal, :] = scipy.stats.zscore(data[sub][nvideo, ncanal, :])\n",
        "#              #data[sub][nvideo, ncanal, :] = data[sub][nvideo, ncanal, :] - data[sub][nvideo, ncanal, :].mean()     \n",
        "\n",
        "# normalize_data3()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy9wCz2YxMwI"
      },
      "source": [
        "type_emotion = 0"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtFNLIZjtn2c",
        "outputId": "e6ff85af-70fb-49fe-884a-f3cb3f968e08"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "k  = 5\n",
        "labels_bin = []\n",
        "for sub in range(32):\n",
        "  temp = labels[sub] >= 4.5\n",
        "  #print(labels[i])\n",
        "  #print(temp)\n",
        "  labels_bin.append(temp)\n",
        "  print(sum(labels_bin[sub][:, type_emotion]), end=' ')\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 28 33 30 26 23 24 27 23 20 20 22 17 26 30 26 28 25 22 32 19 24 27 33 25 23 28 29 22 30 17 29 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9XMfy6AjYS",
        "outputId": "6ed8d9f4-5f25-40dd-e0c9-3a78c8ff8ea0"
      },
      "source": [
        "print(labels_bin[0][:, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True  True  True  True  True  True  True  True False False False False\n",
            " False False False  True False  True  True  True  True  True  True  True\n",
            "  True  True  True False False False False False False False False False\n",
            " False False False  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvgmk-wCwch4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f19d088-6926-4f8d-ccc6-656c879a15ce"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "k  = 5\n",
        "labels_bin = []\n",
        "for sub in range(32):\n",
        "  temp = labels[sub] >= 4.5\n",
        "  #print(labels[i])\n",
        "  #print(temp)\n",
        "  labels_bin.append(temp)\n",
        "  print(sum(labels_bin[sub][:, type_emotion]), end=' ')\n",
        "\n",
        "inds_train = []\n",
        "inds_test = []\n",
        "for sub in range(32):  \n",
        "    X = np.arange(40)\n",
        "    y = np.array(labels_bin[sub][:, type_emotion])\n",
        "    skf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
        "    balanced_split = skf.split(X, y)\n",
        "    for ind_train, ind_test in  balanced_split:\n",
        "        #print(ind_train, ind_test)\n",
        "        #количестов меток 1 в трейне и тесте\n",
        "        #print(sum(labels_bin[sub][ind_train, type_emotion]), sum(labels_bin[sub][ind_test, type_emotion]))\n",
        "        #print((labels_bin[sub][ind_train, type_emotion]), (labels_bin[sub][ind_test, type_emotion]))\n",
        "        inds_train.append(ind_train)\n",
        "        inds_test.append(ind_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 28 33 30 26 23 24 27 23 20 20 22 17 26 30 26 28 25 22 32 19 24 27 33 25 23 28 29 22 30 17 29 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2lPXmuCYbUI"
      },
      "source": [
        "# transforms = [to_segments(),ToTensor()] \n",
        "# train_dataset = EmotionDataset(data, labels_bin, transforms, inds_train)\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "#                                  pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "# val_dataset = EmotionDataset(data, labels_bin, transforms,  inds_test)\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "#                                pin_memory=True, shuffle=False, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXr5pDMn96gt"
      },
      "source": [
        "# sample = val_dataset.__getitem__(7)\n",
        "# print(sample['data'].shape)\n",
        "# print(sample['data'])\n",
        "# print(sample['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbzMK-ZDNXok"
      },
      "source": [
        "writer = None"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRGvG4QaaYl6"
      },
      "source": [
        "def train(model, loader, criterion, optimizer, device, val_dataloader, val_f1_min,  description, batch = None, writer = None):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    inputs = []\n",
        "    #torch.autograd.set_detect_anomaly(True)\n",
        "   \n",
        "    #lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)#, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
        "    for i , batch in enumerate(loader):#, total=len(loader), desc=\"training...\", position=0 , leave = True)):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels'][:, 0]\n",
        "            #print(batch)\n",
        "            #print(trg.shape)\n",
        "            #print(src.shape)\n",
        "            #print(trg.shape)\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "           \n",
        "            #break\n",
        "            #print(levels_pred.shape)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            #print(levels_pred.shape)\n",
        "            loss = criterion(levels_pred, trg) \n",
        "\n",
        "            # loss = 0\n",
        "            # #loss_all = []\n",
        "            \n",
        "            # for class_value in range(2):\n",
        "            #       mask = (trg == class_value) \n",
        "            #       levels_pred_sub = levels_pred[mask]\n",
        "            #       trg_sub = trg[mask]\n",
        "            #       #len += trg.shape[0]\n",
        "            #       #print(f\"{trg.shape[0]}\", end = \" \")\n",
        "            #       if (trg_sub.shape[0] != 0):\n",
        "            #             #loss_all.append(criterion(levels_pred_sub, trg_sub)/trg_sub.shape[0])\n",
        "            #             loss += criterion(levels_pred_sub, trg_sub)/trg_sub.shape[0] \n",
        "            # #loss = loss_all[0] + loss_all[1] + (loss_all[0] - loss_all[1]) ** 2\n",
        "            train_loss.append(loss.item())\n",
        "            #print(train_loss[-1])\n",
        "            loss.backward()\n",
        "            #print(train_loss[-1])\n",
        "            optimizer.step()\n",
        "            if writer:\n",
        "                writer.add_scalar(f'{description}/training loss per batch',\n",
        "                                  loss.item(),\n",
        "                                  i)\n",
        "\n",
        "            # if (i % 100 == 0):\n",
        "            #     acc, f1 = (calculate_predictions(model, val_dataloader))\n",
        "            #     if (f1 > val_f1_min):\n",
        "            #           val_f1_min      = f1\n",
        "            #           torch.save({'model_state_dict': model.state_dict(),    'optimizer_state_dict': optimizer.state_dict(),}, os.path.join(args.output_dir, f\"val.tgz\"))\n",
        "            #break\n",
        "    return np.mean(train_loss), val_f1_min\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiYcR4aI_I_v"
      },
      "source": [
        "def evaluate(model, loader, criterion, device, writer, description):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "  \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for s, batch in enumerate(loader):#, total=len(loader), desc=\"validating...\", position=0 , leave = True)):\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels'][:, 0]\n",
        "            #print(src.shape)\n",
        "\n",
        "\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            #print(levels_pred.shape)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            loss = criterion(levels_pred, trg) \n",
        "            epoch_loss += loss.item() \n",
        "\n",
        "\n",
        "            if writer:\n",
        "                writer.add_scalar(f'{description}/val loss per batch',\n",
        "                                  loss.item(),\n",
        "                                  s)\n",
        "        \n",
        "    return epoch_loss / s"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osXxKvs4nAAQ"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
        "\n",
        "def calculate_predictions(model, loader, output = False):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "    real = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(loader):#, total=len(loader), desc=\"predicting...\", position=0 , leave = True)):\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels'][:, 0]\n",
        "         \n",
        "\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            #print(levels_pred.shape)\n",
        "            trg_pred = levels_pred.argmax(1)\n",
        "            \n",
        "            real.extend(trg)\n",
        "            pred.extend(trg_pred) \n",
        "\n",
        "            \n",
        "        if output == True:    \n",
        "            print(accuracy_score(real, pred)) \n",
        "            print(confusion_matrix(real, pred))  \n",
        "            print(classification_report(real, pred))  \n",
        "        f1 = ((f1_score(real, pred, 'binary', pos_label = 0))  + (f1_score(real, pred, 'binary', pos_label = 1)))/2\n",
        "        return (accuracy_score(real, pred)) , f1\n",
        "        #plt.hist(real)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uxcCqufwbQ9"
      },
      "source": [
        "def train_loop(description, n_epochs = 10):\n",
        "    #args.epochs = 10\n",
        "    #criterion =  fnn.mse_loss\n",
        "    train_loss_min = 10000\n",
        "    val_f1_min = -10000\n",
        "\n",
        "    #batch = next(iter(train_dataloader))\n",
        "    for epoch in range(n_epochs):\n",
        "          #logger.info(f\"Starting epoch {epoch + 1}/{args.epochs}.\")\n",
        "    \n",
        "          train_loss, val_f1_min  = train(model, train_dataloader, criterion, optimizer ,device, val_dataloader, val_f1_min ,  description , None,  writer)\n",
        "          #if epoch % 500 == 0:\n",
        "          if writer:\n",
        "                writer.add_scalar(f\"{description}/training loss per epoch\",\n",
        "                                        train_loss,\n",
        "                                        epoch)\n",
        "          print(train_loss)\n",
        "\n",
        "          if (train_loss < train_loss_min):\n",
        "                 train_loss_min      = train_loss\n",
        "                 torch.save({\n",
        "                         'model_state_dict': model.state_dict(),\n",
        "                         'optimizer_state_dict': optimizer.state_dict(),\n",
        "                       },\n",
        "                       os.path.join(args.output_dir, \"train.tgz\")\n",
        "            )  \n",
        "\n",
        "          val_loss = evaluate(model, val_dataloader, criterion, device,  writer, description )\n",
        "          # #break\n",
        "          print(val_loss)\n",
        "          #if writer:\n",
        "          #      writer.add_scalar(f\"{description}/val loss per epoch\",\n",
        "          #                        val_loss,\n",
        "          #                        epoch)\n",
        "          acc, f1 = (calculate_predictions(model, val_dataloader, False))\n",
        "          print(acc, f1)\n",
        "          if (acc > val_f1_min):\n",
        "                       val_f1_min      = acc\n",
        "                       torch.save({'model_state_dict': model.state_dict(),    'optimizer_state_dict': optimizer.state_dict(),}, os.path.join(args.output_dir, f\"val_{description}.tgz\")) "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfHGjieBBnvq"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89SkPODL-TuZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCIL1fKr-TxK"
      },
      "source": [
        "type_emotion = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL_oAOJ--Up7"
      },
      "source": [
        "# from sklearn.model_selection import StratifiedKFold, KFold \n",
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "# k  = 5\n",
        "# labels_bin = []\n",
        "# for sub in range(32):\n",
        "#   temp = labels[sub] >= 4.5\n",
        "#   #print(labels[i])\n",
        "#   #print(temp)\n",
        "#   labels_bin.append(temp)\n",
        "#   print(sum(labels_bin[sub][:, type_emotion]), end=' ')\n",
        "\n",
        "# inds_train = []\n",
        "# inds_test = []\n",
        "# for sub in range(32):  \n",
        "#     X = np.arange(40)\n",
        "#     y = np.array(labels_bin[sub][:, type_emotion])\n",
        "#     skf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
        "#     balanced_split = skf.split(X, y)\n",
        "#     for ind_train, ind_test in  balanced_split:\n",
        "#         print(ind_train, ind_test)\n",
        "#         #количестов меток 1 в трейне и тесте\n",
        "#         print(sum(labels_bin[sub][ind_train, type_emotion]), sum(labels_bin[sub][ind_test, type_emotion]))\n",
        "#         print((labels_bin[sub][ind_train, type_emotion]), (labels_bin[sub][ind_test, type_emotion]))\n",
        "#         inds_train.append(ind_train)\n",
        "#         inds_test.append(ind_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI84mgp43PWk",
        "outputId": "0705b0a6-8053-49c1-8f1c-31dd58df26d9"
      },
      "source": [
        "print(len(labels_bin))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPqxclY_-Up9"
      },
      "source": [
        "# transforms = [to_segments(),ToTensor()] \n",
        "# train_dataset = EmotionDataset(data, labels_bin, transforms, inds_train)\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "#                                  pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "# val_dataset = EmotionDataset(data, labels_bin, transforms,  inds_test)\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "#                                pin_memory=True, shuffle=False, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGn5BNrB-Tzg"
      },
      "source": [
        "  # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # model = get_model()\n",
        "  # model.apply(initialize_weights)\n",
        "  # criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "  # #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "  # optimizer = optim.Adam(model.parameters(), lr=3e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "  # train_loop('lstm_all_subject',  20)\n",
        "  # description = 'lstm_all_subject'\n",
        "  # model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "  # #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "  # model.load_state_dict(model_state['model_state_dict'])\n",
        "  # acc, f1 = calculate_predictions(model, val_dataloader, True)\n",
        "  # acc_sub.append(acc)\n",
        "  # f1_sub.append(f1)\n",
        "  # print(acc, f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMyBuUketdRj",
        "outputId": "95ead1ef-8242-4431-ee3c-acc25859c0b0"
      },
      "source": [
        "import pandas as pd\n",
        "LEN_RECORD = 60\n",
        "args.batch_size = 4\n",
        "k = 4\n",
        "#transforms_random = RandomAugmentation([add_noise, reset_part_in_freq, reset_part_in_time, None], 0.2)\n",
        "#transforms = [RandomAugmentation([add_noise(), reset_part_in_freq(0.2), reset_part_in_time(0.2), None], 0.2), to_head_matrix(),ToTensor()]   \n",
        "transforms = [to_psd(),ToTensor()] \n",
        "#transforms = [RandomAugmentation([add_noise(), None], 0.5), to_head_matrix(),ToTensor()]   \n",
        "#transforms = [RandomAugmentation([reset_part_in_time(0.4), None], 0.1), to_head_matrix(),ToTensor()]   \n",
        "acc_all = []\n",
        "f1_all = []\n",
        "for sub in range(0, 1): \n",
        "    print(f\"***sub {sub}****\") \n",
        "    X = np.arange(40)\n",
        "    y = np.array(labels_bin[sub][:, type_emotion])\n",
        "    skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=True)\n",
        "    balanced_split = skf.split(X, y)\n",
        "    acc_sub = []\n",
        "    f1_sub = []\n",
        "    for fold, (ind_train, ind_test) in  enumerate(balanced_split):\n",
        "        #print(ind_train, ind_test)\n",
        "        print(f\"***fold {fold}****\") \n",
        "        #количестов меток 1 в трейне и тесте\n",
        "        #print(sum(labels_bin[sub][ind_train, type_emotion]), sum(labels_bin[sub][ind_test, type_emotion]))\n",
        "        #print((labels_bin[sub][ind_train, type_emotion]), (labels_bin[sub][ind_test, type_emotion]))\n",
        "        #ind_train = [ 0,  3,  4,  5,  6,  7,  8, 10, 13, 14, 15, 16, 17, 18, 19, 20, 21,  23, 26, 27, 28, 30, 31, 32, 33, 34, 36, 37, 38, 39]\n",
        "        #ind_test = [ 1,  2,  9, 11, 12, 22, 24, 25, 29, 35]\n",
        "        inds_train = []\n",
        "        inds_test = []\n",
        "        inds_train.append(ind_train)\n",
        "        inds_test.append(ind_test)\n",
        "        print(inds_train)\n",
        "        print(inds_test)\n",
        "        print(labels_bin[sub][inds_test, 0])\n",
        "\n",
        "        train_dataset = EmotionDataset(data[sub : sub + 1 ], labels_bin[sub  : sub + 1], transforms, inds_train, interval = 2, len_seq = 10)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                        pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "        val_dataset = EmotionDataset(data[sub : sub + 1], labels_bin[ sub : sub + 1], transforms,  inds_test, interval = 2, len_seq = 10)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                      pin_memory=True, shuffle=False, drop_last=False)\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        LEN_SEQ = 10\n",
        "        INPUT_SIZE = 1760 \n",
        "        DROPOUT_COEFF = 0.2\n",
        "        HIDDEN_SIZE_2 = 500\n",
        "        HIDDEN_SIZE_1 = 500\n",
        "        model = get_model(INPUT_SIZE, HIDDEN_SIZE_1, HIDDEN_SIZE_2, DROPOUT_COEFF, LEN_SEQ)\n",
        "        model.apply(initialize_weights)\n",
        "        criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "        #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
        "        train_loop('lstm_subject1',  2)\n",
        "        description = 'lstm_subject1'\n",
        "        model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "        #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "        model.load_state_dict(model_state['model_state_dict'])\n",
        "        acc, f1 = calculate_predictions(model, val_dataloader, True)\n",
        "        acc_sub.append(acc)\n",
        "        f1_sub.append(f1)\n",
        "        print(acc, f1)\n",
        "    acc_all.append(acc_sub)    \n",
        "    f1_all.append(f1_sub)  \n",
        "    #f1_data_1 = pd.DataFrame(f1_all, columns = ['f1_fold1', 'f1_fold2', 'f1_fold3', 'f1_fold4', 'f1_fold5'])\n",
        "    #acc_data_1 = pd.DataFrame(acc_all, columns = ['acc_fold1', 'acc_fold2', 'acc_fold3', 'acc_fold4', 'acc_fold5'])\n",
        "    f1_data_1 = pd.DataFrame(f1_all, columns = ['f1_fold1', 'f1_fold2', 'f1_fold3', 'f1_fold4'])\n",
        "    acc_data_1 = pd.DataFrame(acc_all, columns = ['acc_fold1', 'acc_fold2', 'acc_fold3', 'acc_fold4'])\n",
        "    \n",
        "    f1_data_1.to_csv(\"f1_data_1_lstm222.csv\")\n",
        "    acc_data_1.to_csv(\"acc_data_1_lstm222.csv\")  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***sub 0****\n",
            "***fold 0****\n",
            "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 18,\n",
            "       19, 23, 25, 26, 27, 28, 30, 32, 34, 36, 37, 38, 39])]\n",
            "[array([10, 17, 20, 21, 22, 24, 29, 31, 33, 35])]\n",
            "[[False  True  True  True  True  True False False False False]]\n",
            "0.4661395699271162\n",
            "0.8367145152068606\n",
            "0.4926829268292683 0.45333333333333337\n",
            "0.21186552005926154\n",
            "0.8963488197940237\n",
            "0.5097560975609756 0.5087010176524243\n",
            "0.5097560975609756\n",
            "[[ 95 110]\n",
            " [ 91 114]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.46      0.49       205\n",
            "           1       0.51      0.56      0.53       205\n",
            "\n",
            "    accuracy                           0.51       410\n",
            "   macro avg       0.51      0.51      0.51       410\n",
            "weighted avg       0.51      0.51      0.51       410\n",
            "\n",
            "0.5097560975609756 0.5087010176524243\n",
            "***fold 1****\n",
            "[array([ 0,  2,  3,  4,  6,  7,  8, 10, 11, 13, 14, 15, 16, 17, 20, 21, 22,\n",
            "       23, 24, 26, 27, 29, 31, 32, 33, 34, 35, 36, 37, 39])]\n",
            "[array([ 1,  5,  9, 12, 18, 19, 25, 28, 30, 38])]\n",
            "[[ True  True False False  True  True  True False False False]]\n",
            "0.4418032026761712\n",
            "0.8719084004268927\n",
            "0.4170731707317073 0.415539875580792\n",
            "0.2066830743759482\n",
            "0.9239391606556726\n",
            "0.5365853658536586 0.5264437689969605\n",
            "0.5365853658536586\n",
            "[[ 80 125]\n",
            " [ 65 140]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.39      0.46       205\n",
            "           1       0.53      0.68      0.60       205\n",
            "\n",
            "    accuracy                           0.54       410\n",
            "   macro avg       0.54      0.54      0.53       410\n",
            "weighted avg       0.54      0.54      0.53       410\n",
            "\n",
            "0.5365853658536586 0.5264437689969605\n",
            "***fold 2****\n",
            "[array([ 0,  1,  5,  7,  8,  9, 10, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23,\n",
            "       24, 25, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39])]\n",
            "[array([ 2,  3,  4,  6, 11, 13, 16, 26, 32, 34])]\n",
            "[[ True  True  True  True False False False  True False False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZnFzI_I8fUj"
      },
      "source": [
        "\n",
        "args.batch_size = 4\n",
        "k = 3\n",
        "#transforms_random = RandomAugmentation([add_noise, reset_part_in_freq, reset_part_in_time, None], 0.2)\n",
        "#transforms = [RandomAugmentation([add_noise(), reset_part_in_freq(0.2), reset_part_in_time(0.2), None], 0.2), to_head_matrix(),ToTensor()]   \n",
        "transforms = [to_segments(),ToTensor()] \n",
        "#transforms = [RandomAugmentation([add_noise(), None], 0.5), to_head_matrix(),ToTensor()]   \n",
        "#transforms = [RandomAugmentation([reset_part_in_time(0.4), None], 0.1), to_head_matrix(),ToTensor()]   \n",
        "acc_all = []\n",
        "f1_all = []\n",
        "for sub in range(1, 2): \n",
        "    print(f\"***sub {sub}****\") \n",
        "    X = np.arange(40)\n",
        "    y = np.array(labels_bin[sub][:, type_emotion])\n",
        "    skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=True)\n",
        "    balanced_split = skf.split(X, y)\n",
        "    acc_sub = []\n",
        "    f1_sub = []\n",
        "    for fold, (ind_train, ind_test) in  enumerate(balanced_split):\n",
        "        #print(ind_train, ind_test)\n",
        "        print(f\"***fold {fold}****\") \n",
        "        #количестов меток 1 в трейне и тесте\n",
        "        #print(sum(labels_bin[sub][ind_train, type_emotion]), sum(labels_bin[sub][ind_test, type_emotion]))\n",
        "        #print((labels_bin[sub][ind_train, type_emotion]), (labels_bin[sub][ind_test, type_emotion]))\n",
        "        inds_train = []\n",
        "        inds_test = []\n",
        "        inds_val = []\n",
        "\n",
        "        ind_val = random.sample(list(ind_test), 2)\n",
        "        ind_test = list(set(ind_test).difference(ind_val))\n",
        "        print(ind_train, ind_test, ind_val)\n",
        "        inds_train.append(ind_train)\n",
        "        inds_test.append(ind_test)\n",
        "        inds_val.append(ind_val)\n",
        "\n",
        "        print(inds_train)\n",
        "        print(inds_test)\n",
        "        print(labels_bin[sub][inds_test, 0])\n",
        "\n",
        "        train_dataset = EmotionDataset(data[sub : sub + 1 ], labels_bin[sub  : sub + 1], transforms, inds_train)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                        pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "        val_dataset = EmotionDataset(data[sub : sub + 1], labels_bin[ sub : sub + 1], transforms,  inds_test)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                      pin_memory=True, shuffle=False, drop_last=False)\n",
        "        \n",
        "\n",
        "        test_dataset = EmotionDataset(data[sub : sub + 1], labels_bin[ sub : sub + 1], transforms,  inds_val)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=2, num_workers=1,\n",
        "                                      pin_memory=True, shuffle=False, drop_last=False)\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = get_model()\n",
        "        model.apply(initialize_weights)\n",
        "        criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "        #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=3e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        train_loop('lstm_subject1',  20)\n",
        "        description = 'lstm_subject1'\n",
        "        model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "        #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "        model.load_state_dict(model_state['model_state_dict'])\n",
        "        acc, f1 = calculate_predictions(model, test_dataloader, True)\n",
        "        acc_sub.append(acc)\n",
        "        f1_sub.append(f1)\n",
        "        print(acc, f1)\n",
        "    acc_all.append(acc_sub)    \n",
        "    f1_all.append(f1_sub)  \n",
        "    #f1_data_1 = pd.DataFrame(f1_all, columns = ['f1_fold1', 'f1_fold2', 'f1_fold3', 'f1_fold4', 'f1_fold5'])\n",
        "    #acc_data_1 = pd.DataFrame(acc_all, columns = ['acc_fold1', 'acc_fold2', 'acc_fold3', 'acc_fold4', 'acc_fold5'])\n",
        "    f1_data_1 = pd.DataFrame(f1_all, columns = ['f1_fold1', 'f1_fold2', 'f1_fold3', 'f1_fold4'])\n",
        "    acc_data_1 = pd.DataFrame(acc_all, columns = ['acc_fold1', 'acc_fold2', 'acc_fold3', 'acc_fold4'])\n",
        "    \n",
        "    f1_data_1.to_csv(\"f1_data_1_lstm222.csv\")\n",
        "    acc_data_1.to_csv(\"acc_data_1_lstm222.csv\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7xkwolJ6Yr0"
      },
      "source": [
        "f1_data_1 = pd.DataFrame(f1_all, columns = ['f1_fold1', 'f1_fold2', 'f1_fold3', 'f1_fold4'])\n",
        "acc_data_1 = pd.DataFrame(acc_all, columns = ['acc_fold1', 'acc_fold2', 'acc_fold3', 'acc_fold4'])\n",
        "    \n",
        "f1_data_1.to_csv(\"f1_data_1_lstm222.csv\")\n",
        "acc_data_1.to_csv(\"acc_data_1_lstm222.csv\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfxYxlEf6dsW"
      },
      "source": [
        "print(acc_data_1)\n",
        "print(f1_data_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-i6taebwkJv"
      },
      "source": [
        "print(labels_bin[1][[ 9, 12, 15, 16, 19, 25, 34, 35, 38, 39], 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izTjgEJ3dwK3"
      },
      "source": [
        "f1_data_1 = pd.DataFrame(f1_all, columns = ['f1_fold1', 'f1_fold2', 'f1_fold3', 'f1_fold4'])\n",
        "acc_data_1 = pd.DataFrame(acc_all, columns = ['acc_fold1', 'acc_fold2', 'acc_fold3', 'acc_fold4'])\n",
        "    \n",
        "f1_data_1.to_csv(\"f1_data_1_lstm2.csv\")\n",
        "acc_data_1.to_csv(\"acc_data_1_lstm2.csv\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnM20xUFpIOm"
      },
      "source": [
        "import pandas as pd\n",
        "f1_data_1 = pd.DataFrame(f1_all, columns = ['f1_fold1', 'f1_fold2', 'f1_fold3', 'f1_fold4', 'f1_fold5'])\n",
        "acc_data_1 = pd.DataFrame(acc_all, columns = ['acc_fold1', 'acc_fold2', 'acc_fold3', 'acc_fold4', 'acc_fold5'])\n",
        "f1_data_1.to_csv(\"f1_data_1_lstm.csv\")\n",
        "acc_data_1.to_csv(\"acc_data_1_lstm.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8-JvOjE6UW_"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyPiEiCiaoSo"
      },
      "source": [
        "import pandas as pd\n",
        "f1_data_1 = pd.read_csv(\"f1_data_1_lstm1.csv\")\n",
        "acc_data_1 = pd.read_csv(\"acc_data_1_lstm1.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrFjP3yhcWQO"
      },
      "source": [
        "print(acc_data_1[ ['acc_fold1', 'acc_fold2', 'acc_fold3', 'acc_fold4']]. mean(axis = 1).mean())\n",
        "print(f1_data_1[ ['f1_fold1', 'f1_fold2', 'f1_fold3', 'f1_fold4']]. mean(axis = 1).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWsw6PdCv7B3"
      },
      "source": [
        "f1_data_1 = pd.to_csv(\"f1_data_1_lstm.csv\")\n",
        "acc_data_1.to_csv(\"acc_data_1_lstm.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imGgWDkg9QR4"
      },
      "source": [
        "description = 'lstm_subject1'\n",
        "model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "#   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "model.load_state_dict(model_state['model_state_dict'])\n",
        "acc, f1 = calculate_predictions(model, val_dataloader, True)\n",
        "print(acc, f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqtayN169rcg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuetbMDC9LD3"
      },
      "source": [
        "# for i , batch in enumerate(train_dataloader):#, total=len(loader), desc=\"training...\", position=0 , leave = True)):\n",
        "#             model = get_model()\n",
        "#             model.train()\n",
        "#             optimizer.zero_grad()\n",
        "#             src  = batch['data'].to(device)\n",
        "#             #print(src.shape)\n",
        "#             trg = batch['labels'][:, 0]\n",
        "#             #print(batch)\n",
        "#             #print(trg.shape)\n",
        "#             print(type(src))\n",
        "#             print(src.shape)\n",
        "#             levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "           \n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}