{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mykolesiko/eeg_investigation/blob/diplom/MADE_DE_cross_subject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "synthetic-crown"
      },
      "source": [
        "## In this notebook, we will show how to read DE features with python\n",
        "\n",
        "- Author: Wei Liu\n",
        "- Affiliation: [BCMI lab, Shanghai Jiao Tong University, Shanghai, China](http://bcmi.sjtu.edu.cn)\n",
        "- Date: May, 11, 2021"
      ],
      "id": "synthetic-crown"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lightweight-employment",
        "outputId": "c090c221-49bd-42f2-bab7-7839c737b16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.19.5\n",
            "4.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# check the version of these modules\n",
        "print(np.__version__)\n",
        "print(pickle.format_version)"
      ],
      "id": "lightweight-employment"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from  scipy import stats\n",
        "import scipy\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)"
      ],
      "metadata": {
        "id": "l8T8ygGHV3ui"
      },
      "id": "l8T8ygGHV3ui",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self): #(data_path, epoch, batch_siz, image_size, learning_rate, weight_deca, learning_rate, learning_rate_gamma, weight_bce, load, output_dir)\n",
        "    self.epochs = 2\n",
        "    self.batch_size = 8\n",
        "    self.lr= 3e-4\n",
        "    self.weight_decay= 1e-6\n",
        "    self.learning_rate=None\n",
        "    self.learning_rate_gamma=None\n",
        "    self.weight_bce=1\n",
        "    self.load=None\n",
        "    self.output_dir=\"/content/drive/MyDrive/MADE/Project/seed_models/\"\n",
        "    self.data_dir =\"./data_preprocessed_python/\"# \"/content/drive/MyDrive/MADE/Project/train/physionet.org/\"\n",
        "args = Args()   "
      ],
      "metadata": {
        "id": "9CaA_3rnirX_"
      },
      "id": "9CaA_3rnirX_",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bhjiAjAOn93R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/MADE/Project/seed/EEG_DE_features\")"
      ],
      "id": "bhjiAjAOn93R"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVJCHRNXnqVh",
        "outputId": "eba3e1e4-86d7-4873-e4cf-494dd58afc9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "sVJCHRNXnqVh"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "radio-tuner",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45533bbf-7e6d-46a3-f230-4cde8306eef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading 1_123.npz\n",
            "loading 2_123.npz\n",
            "loading 3_123.npz\n",
            "loading 4_123.npz\n",
            "loading 5_123.npz\n",
            "loading 6_123.npz\n",
            "loading 7_123.npz\n",
            "loading 8_123.npz\n",
            "loading 9_123.npz\n",
            "loading 10_123.npz\n",
            "loading 11_123.npz\n",
            "loading 12_123.npz\n",
            "loading 13_123.npz\n",
            "loading 14_123.npz\n",
            "loading 15_123.npz\n"
          ]
        }
      ],
      "source": [
        "# load DE features named '1_123.npz'\n",
        "data_seed = []\n",
        "labels_seed = []\n",
        "for i in range(1, 16):\n",
        "    file_name = f'{i}_123.npz' \n",
        "    print(f\"loading {file_name}\")\n",
        "    data_npz = np.load(file_name)\n",
        "    data_seed.append(pickle.loads(data_npz['data']))\n",
        "    labels_seed.append(pickle.loads(data_npz['label']))\n",
        "    #print(data_npz.files)"
      ],
      "id": "radio-tuner"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "enormous-reward"
      },
      "outputs": [],
      "source": [
        "# As we can see, there are 45 keys in both 'data' and 'label'.\n",
        "# Each participant took part in our experiments for 3 sessions, and he/she watched 15 movie clips (i.e. 15 trials) during each session.\n",
        "# Therefore, we could extract 3 * 15 = 45 DE feature matrices.\n",
        "\n",
        "# The key indexes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] belong to Session 1.\n",
        "# The key indexes [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] belong to Session 2.\n",
        "# The key indexes [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44] belong to Session 3.\n",
        "\n",
        "# We will print the emotion labels for each trial.\n",
        "# label_dict = {0:'Disgust', 1:'Fear', 2:'Sad', 3:'Neutral', 4:'Happy'}\n",
        "# for i in range(45):\n",
        "#     print('Session {} -- Trial {} -- EmotionLabel : {}'.format(i//15+1, i%15+1, label_dict[label[i][0]]))"
      ],
      "id": "enormous-reward"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QKplyttbO7Nc"
      },
      "id": "QKplyttbO7Nc",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "abQ8q-VM1wqy"
      },
      "outputs": [],
      "source": [
        "# def get_label_valence(l):\n",
        "#   if (l == 0) or (l == 1) or (l == 2):\n",
        "#     return 0\n",
        "#   elif (l == 3):\n",
        "#     return 1\n",
        "#   else:\n",
        "#     return 2 \n",
        "\n",
        "# def get_label_valence(l):\n",
        "#    if (l == 0) or (l == 1):\n",
        "#       return 0\n",
        "#    elif (l == 2):\n",
        "#       return 1\n",
        "#    elif (l == 3):\n",
        "#       return 2\n",
        "#    else:\n",
        "#       return 3  \n",
        "\n",
        "def get_label(l):\n",
        "  if (l == 0) or (l == 1) or (l == 2):\n",
        "     return 0\n",
        "  else:  \n",
        "     return 1 "
      ],
      "id": "abQ8q-VM1wqy"
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(15):\n",
        "#   print(labels_seed[i][3][0])"
      ],
      "metadata": {
        "id": "M_8hxH4GgaZD"
      },
      "id": "M_8hxH4GgaZD",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_sub  = []\n",
        "labels_sub = []\n",
        "n_window = 14\n",
        "data_samples_sub = []\n",
        "label_samples_sub = []\n",
        "n_samples = []\n",
        "\n",
        "\n",
        "for sub in range(15):\n",
        "      print(f\"processing {sub}\")\n",
        "      \n",
        "      label = labels_seed[sub]\n",
        "      data = data_seed[sub]\n",
        "      #print(data[0][0, 0])\n",
        "      data_samples = []\n",
        "      label_samples = []\n",
        "      n_data_samples = 0\n",
        "      s = 0\n",
        "      indexes = []\n",
        "      for i in range(45):\n",
        "        n = len(label[i])\n",
        "        print(f\"record{i}-{n}-{int(label[i][0])} \", end='')\n",
        "        for sample, time in enumerate(range(0, n - n_window, n_window)):\n",
        "          if (label[i][0] == 3):# or (label[i][0] == 0) or (label[i][0] == 1):\n",
        "            continue\n",
        "          data_sample = data[i][time : time + n_window]\n",
        "\n",
        "          data_sample = (data_sample - data_sample.mean(axis = 1,  keepdims = True))#/features_all.std(axis = 1,  keepdims = True) + 0.5\n",
        "          data_sample = (data_sample)/data_sample.std(axis = 1,  keepdims = True)\n",
        "          data_sample = data_sample + 0.5\n",
        "          data_sample = data_sample[np.newaxis, :, :]\n",
        "          #print(data_sample.shape)#.transpose(np.newaxis, n_window, -1)\n",
        "          data_samples.append(data_sample)\n",
        "          n_data_samples += 1\n",
        "          label_samples.append(get_label(label[i][0]))\n",
        "      print()    \n",
        "      print(label_samples)  \n",
        "      print(n_data_samples)\n",
        "      print(\"*********************************************\")\n",
        "      n_samples.append(n_data_samples)    \n",
        "      data_samples_sub.append(data_samples)\n",
        "      label_samples_sub.append(label_samples)\n",
        "\n",
        "\n",
        "      \n",
        "      # features_all = (features_all - features_all.mean(axis = 1,  keepdims = True))#/features_all.std(axis = 1,  keepdims = True) + 0.5\n",
        "      # features_all = (features_all)/features_all.std(axis = 1,  keepdims = True)\n",
        "      # features_all = features_all + 0.5\n",
        "      # features_sub.append(features_all)\n",
        "      # labels_sub.append(labels_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCQM1yGnh9Uq",
        "outputId": "39d06dfe-3018-4e37-d748-6230464c561b"
      },
      "id": "RCQM1yGnh9Uq",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing 0\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 1\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 2\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 3\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 4\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 5\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 6\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 7\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 8\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 9\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 10\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 11\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 12\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 13\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n",
            "processing 14\n",
            "record0-18-4 record1-24-1 record2-59-3 record3-46-2 record4-36-0 record5-64-4 record6-74-1 record7-17-3 record8-66-2 record9-35-0 record10-43-4 record11-43-1 record12-58-3 record13-60-2 record14-38-0 record15-59-2 record16-47-1 record17-16-3 record18-31-0 record19-32-4 record20-14-4 record21-60-0 record22-57-3 record23-30-2 record24-24-1 record25-46-3 record26-29-4 record27-23-1 record28-54-2 record29-19-0 record30-72-2 record31-16-1 record32-41-3 record33-22-0 record34-13-4 record35-59-4 record36-21-0 record37-18-3 record38-57-2 record39-71-1 record40-55-3 record41-29-4 record42-51-1 record43-32-2 record44-44-0 \n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "90\n",
            "*********************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(n_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqHWnL6hQ-YF",
        "outputId": "73a02a6a-7132-4b61-8fef-5a1af197eed2"
      },
      "id": "bqHWnL6hQ-YF",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dzv9gR_vK5RP"
      },
      "id": "Dzv9gR_vK5RP",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "\n",
        "    def __init__(self, ):\n",
        "        super(ToTensor, self).__init__()\n",
        "\n",
        "    def __call__(self, sample: dict):\n",
        "        return {\"labels\": torch.tensor(sample[\"labels\"], dtype=torch.float32),\n",
        "                \"data\": torch.tensor(sample[\"data\"], dtype=torch.float32),\n",
        "                } \n",
        "\n",
        "\n",
        "transforms = [ToTensor()]   \n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__ (self, out, n_windows, type):\n",
        "        self.out = out\n",
        "        self.type = type\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.type == 'train':\n",
        "            result = np.sum(n_samples)\n",
        "            #print(result)\n",
        "            result -= n_samples[self.out]\n",
        "\n",
        "        else:\n",
        "            result = n_samples[self.out]  \n",
        "        return result\n",
        "   \n",
        "    def get_index(self, item):\n",
        "        s = 0\n",
        "        if self.type == 'train':\n",
        "          for sub in range(15):\n",
        "              if sub == self.out:\n",
        "                  continue  \n",
        "              if (item >= s) and (item < s + n_samples[sub]):\n",
        "                 return sub, item - s\n",
        "              s += n_samples[sub]\n",
        "        else:\n",
        "           return(self.out, item) \n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "      sample = {}\n",
        "      sub, index = self.get_index(item)\n",
        "      \n",
        "      sample['data'] = data_samples_sub[sub][index]\n",
        "      sample['labels'] = label_samples_sub[sub][index]       \n",
        "      #print(sub, index, sample['labels'] )\n",
        "\n",
        "      if transforms is not None:\n",
        "           for t in transforms:\n",
        "                sample = t(sample)\n",
        "       #print(sample)         \n",
        "      return sample                                      \n",
        "      "
      ],
      "metadata": {
        "id": "8FXkNAB5g0_t"
      },
      "execution_count": 12,
      "outputs": [],
      "id": "8FXkNAB5g0_t"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TzGn1Eh9KcoJ"
      },
      "id": "TzGn1Eh9KcoJ",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# все кроме 5\n",
        "train_dataset = EmotionDataset(5, n_window, 'train')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                   pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "# 5 человек\n",
        "val_dataset = EmotionDataset(5, n_window, 'val')\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, num_workers=1,\n",
        "                                   pin_memory=True, shuffle=False, drop_last=False)\n",
        "        \n",
        " \n",
        " \n"
      ],
      "metadata": {
        "id": "bFRi4L1--cbG"
      },
      "execution_count": 13,
      "outputs": [],
      "id": "bFRi4L1--cbG"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(val_dataloader):\n",
        "        continue"
      ],
      "metadata": {
        "id": "1-Aj7o-t_PbJ"
      },
      "execution_count": 14,
      "outputs": [],
      "id": "1-Aj7o-t_PbJ"
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "for i, batch in enumerate(train_dataloader):#, t\n",
        "        s += 1\n",
        "        continue\n",
        "print(f\"**********{s}***********\")        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjmshoLtD8Np",
        "outputId": "45220807-2523-4131-bfd6-6af032f9a0c2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********157***********\n"
          ]
        }
      ],
      "id": "mjmshoLtD8Np"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bearing-april"
      },
      "source": [
        "The above emotion labels should be the same as labels in file \"emotion_label_and_stimuli_order.xlsx\""
      ],
      "id": "bearing-april"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pBogIdaaZQ7G"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold , KFold\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "import pandas as pd\n"
      ],
      "id": "pBogIdaaZQ7G"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j28G3qN02K_r"
      },
      "id": "j28G3qN02K_r",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtbgVviw2LQ8"
      },
      "source": [
        "class EmotionNet(torch.nn.Module): \n",
        "   def __init__(self):\n",
        "      super().__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 32, kernel_size = 3)#, padding='same')\n",
        "      self.relu1 = torch.nn.ReLU()\n",
        "      self.pool1 = torch.nn.MaxPool2d(kernel_size = 2, stride=2)\n",
        "      self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, stride=2)#, padding='same')\n",
        "      self.relu2 = torch.nn.ReLU()\n",
        "      self.pool2 = torch.nn.MaxPool2d(kernel_size = 2, stride=2)\n",
        "      \n",
        "      self.flat = torch.nn.Flatten(1, 3)\n",
        "      #self.fc1 = torch.nn.Linear(512, 1024)\n",
        "      self.fc1 = torch.nn.Linear(2432, 1024)\n",
        "      self.relu3 = torch.nn.ReLU()\n",
        "      self.fc2 = torch.nn.Linear(1024, 1)\n",
        "      self.sig = nn.Sigmoid()\n",
        "      \n",
        "   def forward(self, input):\n",
        "      output = self.conv1(input)\n",
        "      #print(output.shape)\n",
        "      output = self.relu1(output)\n",
        "      output = self.pool1(output)\n",
        "      #print(output.shape)\n",
        "      output = self.conv2(output)\n",
        "      #print(output.shape)\n",
        "      output = self.relu2(output)\n",
        "      output = self.pool2(output)\n",
        "      #print(output.shape)\n",
        "      #print(output.shape)\n",
        "      output = self.flat(output)\n",
        "      output = self.fc1(output)\n",
        "      output = self.relu3(output)\n",
        "      output = self.fc2(output)\n",
        "      output = self.sig(output)\n",
        "      #print(output.shape)\n",
        "      \n",
        "      return output.squeeze(1)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [],
      "id": "GtbgVviw2LQ8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y86cOSsmHM9Q"
      },
      "source": [
        "def train(model, loader, criterion, optimizer, device, type_emotion, batch = None):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    inputs = []\n",
        "   \n",
        "    #lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)#, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
        "    #for batch in tqdm(loader, total=len(loader), desc=\"training...\", position=0 , leave = True):\n",
        "    for s, batch in enumerate(loader):#, t\n",
        "    \n",
        "            optimizer.zero_grad()\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels']\n",
        "\n",
        "            #print(batch)\n",
        "            #print(trg.shape)\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            #print(levels_pred.shape)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "\n",
        "            #usual cross entropy\n",
        "            #output = levels_pred[:, 1:].reshape(-1, levels_pred.shape[-1])\n",
        "            #trg1 = trg[:, 1:].reshape(-1)\n",
        "            #print(levels_pred.shape)\n",
        "            #print(trg.shape)\n",
        "            loss = criterion(levels_pred, trg) \n",
        "\n",
        "            #print(\"after\")\n",
        "            train_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #break\n",
        "    return np.mean(train_loss)#, mid_outputs"
      ],
      "execution_count": 18,
      "outputs": [],
      "id": "Y86cOSsmHM9Q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltPJdcApHQ9Q"
      },
      "source": [
        "def evaluate(model, loader, criterion, device, type_emotion):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "  \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        #for s, batch in enumerate(tqdm(loader, total=len(loader), desc=\"validating...\", position=0 , leave = True)):\n",
        "        for s, batch in enumerate(loader):#, t\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels']\n",
        "\n",
        "\n",
        "\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            #print(levels_pred.shape)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "\n",
        "            \n",
        "            loss = criterion(levels_pred, trg) \n",
        "\n",
        "            epoch_loss += loss.item() \n",
        "        \n",
        "    return epoch_loss / s\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report, f1_score"
      ],
      "execution_count": 19,
      "outputs": [],
      "id": "ltPJdcApHQ9Q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K3lPyI-HUpR"
      },
      "source": [
        "def calculate_predictions(model, loader, type_emotion, show):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "    real = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        #for i, batch in enumerate(tqdm(loader, total=len(loader), desc=\"predicting...\", position=0 , leave = True)):\n",
        "        for i, batch in enumerate(loader):#, t\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels']\n",
        "           \n",
        "\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            #print(levels_pred.shape)\n",
        "            \n",
        "            #trg_pred = levels_pred.argmax(1)\n",
        "            trg_pred = levels_pred > 0.5\n",
        "            \n",
        "            real.extend(trg)\n",
        "            pred.extend(trg_pred) \n",
        "\n",
        "        if show:    \n",
        "          print(accuracy_score(real, pred)) \n",
        "          print(confusion_matrix(real, pred))  \n",
        "          print(classification_report(real, pred))   \n",
        "        #print(real)\n",
        "        #print(pred)  \n",
        "    f1 = ((f1_score(real, pred, average = 'binary', pos_label = 0))  + (f1_score(real, pred, average = 'binary', pos_label = 1)))/2\n",
        "    return (accuracy_score(real, pred)) , f1        \n",
        "    "
      ],
      "execution_count": 20,
      "outputs": [],
      "id": "1K3lPyI-HUpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy-tCTeIfzIt",
        "outputId": "46c57fc0-1c67-4763-899d-33988e5f84c5"
      },
      "source": [
        "def get_model():\n",
        "  model = EmotionNet().to(device)\n",
        "  return model\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_model()\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EmotionNet(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "  (relu2): ReLU()\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flat): Flatten(start_dim=1, end_dim=3)\n",
              "  (fc1): Linear(in_features=2432, out_features=1024, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc2): Linear(in_features=1024, out_features=1, bias=True)\n",
              "  (sig): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "id": "Cy-tCTeIfzIt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOT0qjQSLPUy"
      },
      "source": [
        "def train_loop(type_emotion, description , nepochs = 5):\n",
        "        args.epochs = nepochs\n",
        "        #criterion =  fnn.mse_loss\n",
        "        train_loss_min = 10000\n",
        "        f1_min = -10000\n",
        "        #batch = next(iter(train_dataloader))\n",
        "        for epoch in range(args.epochs):\n",
        "            #logger.info(f\"Starting epoch {epoch + 1}/{args.epochs}.\")\n",
        "            \n",
        "            train_loss = train(model, train_dataloader, criterion, optimizer ,device, type_emotion)\n",
        "            #if epoch % 500 == 0:\n",
        "            #print(f\"train_loss = {train_loss}\")\n",
        "\n",
        "            if (train_loss < train_loss_min):\n",
        "                train_loss_min      = train_loss\n",
        "                torch.save({\n",
        "                                'model_state_dict': model.state_dict(),\n",
        "                                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                              },\n",
        "                              os.path.join(\"/content/drive/MyDrive/MADE/Project/CNN_models/\", \"train.tgz\")\n",
        "                    )  \n",
        "\n",
        "            #val_loss = evaluate(model, val_dataloader, criterion, device, type_emotion)\n",
        "            # #break\n",
        "            #print(val_loss)\n",
        "            #break\n",
        "\n",
        "            acc, f1 = calculate_predictions(model, val_dataloader, type_emotion, False)\n",
        "            #print(f1, acc)\n",
        "            if (f1 > f1_min):\n",
        "              f1_min      =  f1\n",
        "              torch.save({'model_state_dict': model.state_dict(),    'optimizer_state_dict': optimizer.state_dict(),}, os.path.join(\"/content/drive/MyDrive/MADE/Project/seed_models\", f\"{description}.tgz\"))"
      ],
      "execution_count": 22,
      "outputs": [],
      "id": "mOT0qjQSLPUy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usRm7eXzpJXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6eaecb-4429-4369-83b0-07d2ce304bcc"
      },
      "source": [
        "type_emotion = 0\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "n_window = 14\n",
        "args.batch_size = 128\n",
        "\n",
        "acc_all = []\n",
        "f1_all = []\n",
        "\n",
        "for sub in range(0, 15): \n",
        "    print(sub) \n",
        "    train_dataset = EmotionDataset(sub, n_window, 'train')\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                   pin_memory=True, shuffle=True, drop_last=True)\n",
        "    val_dataset = EmotionDataset(sub, n_window, 'val')\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=4, num_workers=1,\n",
        "                                   pin_memory=True, shuffle=False, drop_last=False)\n",
        "        \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = get_model()\n",
        "    model.apply(initialize_weights)\n",
        "    #weight = torch.FloatTensor([1/27, 1/6])\n",
        "   # criterion = nn.CrossEntropyLoss(weight = weight, reduction = 'mean')#torch.nn.MSELoss()\n",
        "    #criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "    criterion = nn.MSELoss()\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "    description = f'seed_common_{sub}'\n",
        "    train_loop(type_emotion, description , 200)\n",
        "\n",
        "\n",
        "    model_state  = torch.load(os.path.join(\"/content/drive/MyDrive/MADE/Project/seed_models/\",  f\"{description}.tgz\"))\n",
        "    #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "    model.load_state_dict(model_state['model_state_dict'])\n",
        "    \n",
        "    acc, f1 = calculate_predictions(model, val_dataloader, type_emotion, True)\n",
        "    print(f\"f1 = {f1} acc = {acc}\")\n",
        "    acc_all.append(acc)\n",
        "    f1_all.append(f1)\n",
        "    print(acc, f1)\n",
        "    pd.DataFrame(f1_all).to_csv(\"f1_data_seed_result.csv\")\n",
        "    pd.DataFrame(acc_all).to_csv(\"acc_data_seed_result.csv\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0.8555555555555555\n",
            "[[68  4]\n",
            " [ 9  9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.94      0.91        72\n",
            "         1.0       0.69      0.50      0.58        18\n",
            "\n",
            "    accuracy                           0.86        90\n",
            "   macro avg       0.79      0.72      0.75        90\n",
            "weighted avg       0.84      0.86      0.85        90\n",
            "\n",
            "f1 = 0.7466984195713358 acc = 0.8555555555555555\n",
            "0.8555555555555555 0.7466984195713358\n",
            "1\n",
            "0.9111111111111111\n",
            "[[71  1]\n",
            " [ 7 11]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.99      0.95        72\n",
            "         1.0       0.92      0.61      0.73        18\n",
            "\n",
            "    accuracy                           0.91        90\n",
            "   macro avg       0.91      0.80      0.84        90\n",
            "weighted avg       0.91      0.91      0.90        90\n",
            "\n",
            "f1 = 0.8400000000000001 acc = 0.9111111111111111\n",
            "0.9111111111111111 0.8400000000000001\n",
            "2\n",
            "0.9888888888888889\n",
            "[[72  0]\n",
            " [ 1 17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      0.99        72\n",
            "         1.0       1.00      0.94      0.97        18\n",
            "\n",
            "    accuracy                           0.99        90\n",
            "   macro avg       0.99      0.97      0.98        90\n",
            "weighted avg       0.99      0.99      0.99        90\n",
            "\n",
            "f1 = 0.9822660098522167 acc = 0.9888888888888889\n",
            "0.9888888888888889 0.9822660098522167\n",
            "3\n",
            "0.7777777777777778\n",
            "[[57 15]\n",
            " [ 5 13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.79      0.85        72\n",
            "         1.0       0.46      0.72      0.57        18\n",
            "\n",
            "    accuracy                           0.78        90\n",
            "   macro avg       0.69      0.76      0.71        90\n",
            "weighted avg       0.83      0.78      0.79        90\n",
            "\n",
            "f1 = 0.7079818299805322 acc = 0.7777777777777778\n",
            "0.7777777777777778 0.7079818299805322\n",
            "4\n",
            "0.8\n",
            "[[72  0]\n",
            " [18  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      1.00      0.89        72\n",
            "         1.0       0.00      0.00      0.00        18\n",
            "\n",
            "    accuracy                           0.80        90\n",
            "   macro avg       0.40      0.50      0.44        90\n",
            "weighted avg       0.64      0.80      0.71        90\n",
            "\n",
            "f1 = 0.4444444444444445 acc = 0.8\n",
            "0.8 0.4444444444444445\n",
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8111111111111111\n",
            "[[72  0]\n",
            " [17  1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      1.00      0.89        72\n",
            "         1.0       1.00      0.06      0.11        18\n",
            "\n",
            "    accuracy                           0.81        90\n",
            "   macro avg       0.90      0.53      0.50        90\n",
            "weighted avg       0.85      0.81      0.74        90\n",
            "\n",
            "f1 = 0.4998365478914678 acc = 0.8111111111111111\n",
            "0.8111111111111111 0.4998365478914678\n",
            "6\n",
            "0.8\n",
            "[[72  0]\n",
            " [18  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      1.00      0.89        72\n",
            "         1.0       0.00      0.00      0.00        18\n",
            "\n",
            "    accuracy                           0.80        90\n",
            "   macro avg       0.40      0.50      0.44        90\n",
            "weighted avg       0.64      0.80      0.71        90\n",
            "\n",
            "f1 = 0.4444444444444445 acc = 0.8\n",
            "0.8 0.4444444444444445\n",
            "7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "[[72  0]\n",
            " [ 0 18]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00        72\n",
            "         1.0       1.00      1.00      1.00        18\n",
            "\n",
            "    accuracy                           1.00        90\n",
            "   macro avg       1.00      1.00      1.00        90\n",
            "weighted avg       1.00      1.00      1.00        90\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "8\n",
            "0.9444444444444444\n",
            "[[68  4]\n",
            " [ 1 17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.94      0.96        72\n",
            "         1.0       0.81      0.94      0.87        18\n",
            "\n",
            "    accuracy                           0.94        90\n",
            "   macro avg       0.90      0.94      0.92        90\n",
            "weighted avg       0.95      0.94      0.95        90\n",
            "\n",
            "f1 = 0.9181669394435352 acc = 0.9444444444444444\n",
            "0.9444444444444444 0.9181669394435352\n",
            "9\n",
            "0.8222222222222222\n",
            "[[71  1]\n",
            " [15  3]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.99      0.90        72\n",
            "         1.0       0.75      0.17      0.27        18\n",
            "\n",
            "    accuracy                           0.82        90\n",
            "   macro avg       0.79      0.58      0.59        90\n",
            "weighted avg       0.81      0.82      0.77        90\n",
            "\n",
            "f1 = 0.5857307249712314 acc = 0.8222222222222222\n",
            "0.8222222222222222 0.5857307249712314\n",
            "10\n",
            "0.8888888888888888\n",
            "[[72  0]\n",
            " [10  8]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      1.00      0.94        72\n",
            "         1.0       1.00      0.44      0.62        18\n",
            "\n",
            "    accuracy                           0.89        90\n",
            "   macro avg       0.94      0.72      0.78        90\n",
            "weighted avg       0.90      0.89      0.87        90\n",
            "\n",
            "f1 = 0.7752247752247752 acc = 0.8888888888888888\n",
            "0.8888888888888888 0.7752247752247752\n",
            "11\n",
            "0.8222222222222222\n",
            "[[70  2]\n",
            " [14  4]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.97      0.90        72\n",
            "         1.0       0.67      0.22      0.33        18\n",
            "\n",
            "    accuracy                           0.82        90\n",
            "   macro avg       0.75      0.60      0.62        90\n",
            "weighted avg       0.80      0.82      0.78        90\n",
            "\n",
            "f1 = 0.6153846153846154 acc = 0.8222222222222222\n",
            "0.8222222222222222 0.6153846153846154\n",
            "12\n",
            "0.9444444444444444\n",
            "[[70  2]\n",
            " [ 3 15]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.97      0.97        72\n",
            "         1.0       0.88      0.83      0.86        18\n",
            "\n",
            "    accuracy                           0.94        90\n",
            "   macro avg       0.92      0.90      0.91        90\n",
            "weighted avg       0.94      0.94      0.94        90\n",
            "\n",
            "f1 = 0.9113300492610837 acc = 0.9444444444444444\n",
            "0.9444444444444444 0.9113300492610837\n",
            "13\n",
            "0.7888888888888889\n",
            "[[65  7]\n",
            " [12  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.90      0.87        72\n",
            "         1.0       0.46      0.33      0.39        18\n",
            "\n",
            "    accuracy                           0.79        90\n",
            "   macro avg       0.65      0.62      0.63        90\n",
            "weighted avg       0.77      0.79      0.78        90\n",
            "\n",
            "f1 = 0.6297899978350292 acc = 0.7888888888888889\n",
            "0.7888888888888889 0.6297899978350292\n",
            "14\n",
            "0.8666666666666667\n",
            "[[69  3]\n",
            " [ 9  9]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.96      0.92        72\n",
            "         1.0       0.75      0.50      0.60        18\n",
            "\n",
            "    accuracy                           0.87        90\n",
            "   macro avg       0.82      0.73      0.76        90\n",
            "weighted avg       0.86      0.87      0.86        90\n",
            "\n",
            "f1 = 0.76 acc = 0.8666666666666667\n",
            "0.8666666666666667 0.76\n"
          ]
        }
      ],
      "id": "usRm7eXzpJXp"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MADE_DE_cross_subject1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}